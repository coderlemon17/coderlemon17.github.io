<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Optimal Transport入门简述 | Lemon's Blog</title><meta name=keywords content="Optimal Transport"><meta name=description content="0. Main Takeaway 本文主要是对Notes on Optimal Transport 的翻译及整理, 同时重构了原文提供的代码, 整理到了一个Jupyter Notebook中并增加了注释, 方便理解"><meta name=author content="Xinning Zhou"><link rel=canonical href=https://coderlemon17.github.io/posts/2022/07-16-ot/><link crossorigin=anonymous href=/assets/css/stylesheet.min.f62327f3747482455304c712f4fc419bbe4fb8f71c7691c5081f28f3c3a6f8e4.css integrity="sha256-9iMn83R0gkVTBMcS9PxBm75PuPccdpHFCB8o88Om+OQ=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.e85ad0406048e8176e1c7661b25d5c69297ddfe41dc4124cf75ecb99a4f7b3d1.js integrity="sha256-6FrQQGBI6BduHHZhsl1caSl93+QdxBJM917LmaT3s9E=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://coderlemon17.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://coderlemon17.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://coderlemon17.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://coderlemon17.github.io/apple-touch-icon.png><link rel=mask-icon href=https://coderlemon17.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.1/es5/tex-mml-chtml.min.js></script>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Optimal Transport入门简述"><meta property="og:description" content="0. Main Takeaway 本文主要是对Notes on Optimal Transport 的翻译及整理, 同时重构了原文提供的代码, 整理到了一个Jupyter Notebook中并增加了注释, 方便理解"><meta property="og:type" content="article"><meta property="og:url" content="https://coderlemon17.github.io/posts/2022/07-16-ot/"><meta property="og:image" content="https://coderlemon17.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-07-16T14:26:33+08:00"><meta property="article:modified_time" content="2022-07-16T14:26:33+08:00"><meta property="og:site_name" content="Lemon's Blog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://coderlemon17.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Optimal Transport入门简述"><meta name=twitter:description content="0. Main Takeaway 本文主要是对Notes on Optimal Transport 的翻译及整理, 同时重构了原文提供的代码, 整理到了一个Jupyter Notebook中并增加了注释, 方便理解"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://coderlemon17.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Optimal Transport入门简述","item":"https://coderlemon17.github.io/posts/2022/07-16-ot/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Optimal Transport入门简述","name":"Optimal Transport入门简述","description":"0. Main Takeaway 本文主要是对Notes on Optimal Transport 的翻译及整理, 同时重构了原文提供的代码, 整理到了一个Jupyter Notebook中并增加了注释, 方便理解","keywords":["Optimal Transport"],"articleBody":"0. Main Takeaway 本文主要是对Notes on Optimal Transport 的翻译及整理, 同时重构了原文提供的代码, 整理到了一个Jupyter Notebook中并增加了注释, 方便理解 (见我的github).\n本文看完可以了解到\nSec 1.: 什么是最优输运; Wasserstein metric; Sinkhorn distance; Sinkhorn-Knopp算法 Sec 2.: 最优输运的实际运用(含代码), 包括训练集和测试集分布不一致时如何插值; 风格迁移. Sec 3.: 提供了整理后的代码. 1. 从一次聚会讲起 It’s Party Time! 假设实验室要举办一场聚会, 一共有n=8个人, 有m=5种小吃. 我们用$\\mathbf{r} = (3,3,4,2,2,2,1)^{\\top}\\in \\mathbb{R}^n$来代表每个人的食量, 用$\\mathbf{c}=(4,2,6,4,4)^{\\top}\\in \\mathbb{R}^m$来代表每个小吃的份数, 如下图:\n同时, 为了让每个人都吃到自己最喜欢的小吃, 我们调查了每个人对每种小吃的喜爱程度, 如下表, $+2$表示最喜欢, $-2$表示最讨厌.\nmerveilleux eclair chocolate mousse bavarois carrot cake Bernard 2 2 1 0 0 Jan 0 -2 -2 -2 2 Willem 1 2 2 2 -1 Hilde 2 1 0 1 -1 Steffie 0.5 2 2 1 0 Marlies 0 1 1 1 -1 Tim -2 2 2 1 1 Wouter 2 1 2 1 -1 接下来问题来了, 我们要如何分配我们的小吃, 从而保证在所有人都吃饱, 且所有小吃都被分完的前提下, 每个人都尽可能吃到自己最喜欢吃的小吃呢?\n1.1. 最优输运 (Optimal Transport) 回顾前文, 我们用$\\mathbf{r} = (3,3,4,2,2,2,1)^{\\top}\\in \\mathbb{R}^n$来代表每个人的食量, 用$\\mathbf{c}=(4,2,6,4,4)^{\\top}\\in \\mathbb{R}^m$来代表每个小吃的份数(我们假设份数是可被连续分割的). 则我们的目标是求解一个维度为$n\\times m$的矩阵$\\mathbf{P}$, 其中$\\mathbf{P}_{ij}$代表第$i$个人应该被分到几份第$j$个小吃.\n我们用$U(\\mathbf{r}, \\mathbf{c})$用来表示所有可能的解空间 (即$\\mathbf{P}$可能的取值空间):\n$$ U(\\mathbf{r}, \\mathbf{c}) = \\{ \\mathbf{P} \\in \\mathbb{R}^{n\\times m}_{\\geq 0}\\mid \\mathbf{P}\\mathbf{1}_m = \\mathbf{r}, \\mathbf{P}^{\\top} \\mathbf{1}_n =\\mathbf{c}\\}. $$\n出于惯例, 我们把每个人对小吃的喜好矩阵取反后得到厌恶/代价矩阵 (cost matrix), 并记为$\\mathbf{M}\\in \\mathbb{R}^{n\\times m}$, 则我们的优化任务为:\n$$ d_\\mathbf{M}(\\mathbf{r}, \\mathbf{c}) = \\min\\limits_{\\mathbf{P}\\in U(\\mathbf{r}, \\mathbf{c})}\\sum\\limits_{ij}\\mathbf{P}_{ij}\\mathbf{M}_{ij} $$\n这便被称之为$\\mathbf{r}$和$\\mathbf{c}$之间的最优输运问题. 注意目标函数和限制函数都是线性的, 所以这个最优输运问题可以被线性规划有效求解 (例如单纯形法, 对偶单纯形法等).其中该问题的最优解$d_{\\mathbf{M}}(\\mathbf{r}, \\mathbf{c})$也被称之为Wasserstein distance, 当$\\mathbf{r}$, $\\mathbf{c}$都是概率分布时, Wasserstein distance便是这两个分布之间的距离的一种度量 (常见的度量还有KL-Divergence, TV-Divergence等).\nOptimal Transport有时候也被称为earth mover distance, 因为当你把$\\mathbf{r}$, $\\mathbf{c}$想象成两座小沙丘时, $\\mathbf{P}_{ij}^{*}$其实描述了你要怎么搬运沙子, 从而以最小的代价让$\\mathbf{r}$变成$\\mathbf{c}$. 举例\n0.2 0.6 0.2 0.6 0 0.6 0 0.4 0.2 0 0.2 假设$\\mathbf{r} = (0.6, 0.4)^{\\top}$, $\\mathbf{c} = (0.2, 0.6, 0.2)^{\\top}$, 我们先不考虑代价矩阵$\\mathbf{M}$. 上表就描述了一种搬运方式: 即把$\\mathbf{r}$的第一堆沙子全搬到$\\mathbf{c}$的第二堆, 把$\\mathbf{r}$的第二堆沙子平均搬到$\\mathbf{c}$的第一堆和第三堆.\nSinkhorn distance和Sinkhorn-Knopp算法 对于一般的最优输运问题, 我们可以使用线性规划求解, 但是当$n$和$m$较大时, 求解会十分耗时, 于是我们不妨考虑一个\"更难\"的加了正则项问题:\n$$ d^{\\lambda}_\\mathbf{M}(\\mathbf{r}, \\mathbf{c}) = \\min\\limits_{\\mathbf{P}\\in U(\\mathbf{r}, \\mathbf{c})}\\sum\\limits_{ij}\\mathbf{P}_{ij}\\mathbf{M}_{ij} - \\frac{1}{\\lambda} h(\\mathbf{P}), \\lambda \u003e 0. $$\n这里$h(\\mathbf{P})$是矩阵$\\mathbf{P}$的信息熵矩阵 (可以先normalize $\\mathbf{P}$再求):\n$$ h(\\mathbf{P}) = -\\sum\\limits_{ij} \\mathbf{P}_{ij}\\log \\mathbf{P}_{ij} $$\n这里引入的正则项$h(\\mathbf{P})$越大, 表示分配矩阵$\\mathbf{P}$越均匀: 即每个人会越均匀的分配到每个小吃, 而$\\lambda$作为超参, 用来平衡求解最优输运和让分配矩阵$\\mathbf{P}$越均匀.\n若我们记$\\mathbf{P}^{*} _{\\lambda}$为上述问题对应的最优解, 则$\\sum\\limits_{ij}(\\mathbf{P}^{*}_{\\lambda})_{ij}\\mathbf{M}_{ij}$被称为Sinkhorn distance. (注意: 正则项$h(\\mathbf{P})$的值是不被计入Sinkhorn distance的.) 在一些问题中, 使用Sinkhorn distance得到的结果会比Wasserstein distance要更好, 因为引入$h(\\mathbf{P})$相当于引入了一个先验: 即在不考虑代价矩阵的情况下, 我们希望每个人分到的越均匀越好.\n虽然一般来说, 加入正则项后会让问题的求解更复杂, 但是对于Sinkhorn distance则是一个例外, 事实上, 我们有一个非常高效且简洁的算法用来求解Sinkhorn distance, 即Sinkhorn-Knopp算法.\n该算法的正确性我们这里不做展开, 感兴趣的同学可以自行查阅《the sinkhorn-knopp algorithm: convergence and applications》.\nSinkhorn-Knopp的算法基于一个发现, $d^{\\lambda}_\\mathbf{M}(\\mathbf{r}, \\mathbf{c})$的最优解$\\mathbf{P}^{*} _{\\lambda}$一定满足如下形式:\n$$ \\left(\\mathbf{P}_{\\lambda}^{\\star}\\right)_{i j}=\\alpha_{i} \\beta_{j} e^{-\\lambda \\mathbf{M}_{i j}} $$\n其中$\\alpha_i$, $\\beta_j$是待求解的常数, 从而保证$\\mathbf{P}_{\\lambda}^{\\star}$横向求和是$\\mathbf{r}$, 纵向求和是$\\mathbf{c}$. 因此我们采用一种迭代的思想去求解, 即\nInput: $\\mathbf{M}, \\mathbf{r}, \\mathbf{c}, \\lambda$\nInitialize: $(\\mathbf{P}_\\lambda)_{ij} = e^{-\\lambda \\mathbf{M}_{ij}}$\nLoop:\nScale每一行, 从而横向求和是$\\mathbf{r}$. Scale每一列, 从而纵向求和是$\\mathbf{c}$. Until convergence.\n下面给出对应的Python实现:\ndef compute\\_optimal\\_transport(M, r, c, lam, epsilon=1e-5): \"\"\" Computes the optimal transport matrix and Slinkhorn distance using the Sinkhorn-Knopp algorithm Inputs: - M : cost matrix (n x m) - r : vector of marginals (n, ) - c : vector of marginals (m, ) - lam : strength of the entropic regularization - epsilon : convergence parameter Output: - P : optimal transport matrix (n x m) - dist : Sinkhorn distance \"\"\" n, m = M.shape P = np.exp(- lam * M) # Avoiding poor math condition P /= P.sum() u = np.zeros(n) # Normalize this matrix so that P.sum(1) == r, P.sum(0) == c while np.max(np.abs(u - P.sum(1))) \u003e epsilon: # Shape (n, ) u = P.sum(1) P *= (r / u).reshape((-1, 1)) P *= (c / P.sum(0)).reshape((1, -1)) return P, np.sum(P * M) 借助Sinkhorn-Knopp算法, 我们可以求解上述加入了entropy正则项的派对问题, 结果如下:\n可以注意到, 当$\\lambda$的值越小, 我们加入的惩罚项$\\frac{1}{\\lambda} h(\\mathbf{P})$会使得最终解更偏向homogeneous的分布, 即每个人分到的小吃会更加均匀, 这也意味着每个人可能会被更多地分到自己不喜欢吃的小吃.\n事实上, 最优输运有着一个很优雅的几何解释: 上图中, $U(\\mathbf{r}, \\mathbf{c})$包含了所有可行解, 而代价矩阵$\\mathbf{M}$则给定了一个方向, 从而指明了什么样的解$\\mathbf{P}$是更优的. 在没有正则项$\\frac{1}{\\lambda} h(\\mathbf{P})$的情况下, 最优输运问题的解会在$U(\\mathbf{r}, \\mathbf{c})$的一个极点取到 (别忘了最优输运本质上是一个线性规划问题). 而在加入正则项$\\frac{1}{\\lambda} h(\\mathbf{P})$后, 我们希望得到的是一个更smooth的解, 例如图中红色椭圆与黄线相切的位置. 这也就解释了为什么加入正则项后求解会更加容易, 因为我们可以不用再管$U(\\mathbf{r}, \\mathbf{c})$复杂的边界了.\n在极端情况下, 当$\\lambda \\rightarrow \\infty$, $\\mathbf{P}_\\lambda^{*}$会无限接近$\\mathbf{P}^{*}$ (直到出现数值不稳定性). 而当$\\lambda \\rightarrow 0$的时候, 我们只会考虑最大化entropy项$h(\\mathbf{P})$, 从而$\\mathbf{P}^{*}_{0} =\\mathbf{r}^{\\top}\\mathbf{c}$. (这是满足约束条件的可行解中entropy最大的解.)\n2. 最优输运的应用实例(含代码) 在了解了最优输运的原理后, 我们给出几个实例: 包括分布匹配, 风格迁移. 代码即注释见github.\n2.1. 分布匹配 分布匹配所关心的问题是, 当我们有两个不同的分布$p_1(X)$和$p_2(X)$, 我们要如何修改分布$p_1$使得其和$p_2$尽可能接近. 这听起来可能有些抽象, 我们不妨考虑一个很具体的例子: 假设我们要做一个目标检测的任务, 但是我们训练集全都是白天的图像, 而测试集全都是黑夜的图像, 那我们要如何调整训练集, 从而使得我们的模型最终能泛化到黑夜上呢?\nNote:\n假设我们有数据集$\\left \\{ x_i\\right \\}_{i=1,\\cdots, N}$, 则它其实就定义了一个分布, 一般称为经验分布, 该分布的概率密度函数为:\n$$ p(X=x) = \\frac{1}{N}\\sum_i^N \\mathbf{1}_{x_i = x} $$\n所以两个数据集之间的差异就可以被转化为两个分布间的差异问题.\nDomain Adaptation 假设我们有两个数据集, 分别为$X_1$ (蓝色) 和 $X_2$ (橘色), 其中$X_1$有$n$个点, $X_2$有$m$个点. 我们不妨就认为$X_1$是白天的图像, $X_2$是夜晚的图像.\n注意这里$\\mathbf{r}$和$\\mathbf{c}$都是均匀分布 ( $p(X=x) = \\frac{1}{N}\\sum\\limits_i^N \\mathbf{1}_{x_i = x}$, 我们假设图片之间两两不同 ). 对于代价矩阵$\\mathbf{M}$, $\\mathbf{M}_{ij}$代表了把图$i$变成图$j$的代价, 这里我们就取Euclidean Distance, 即$\\lVert x_i -x_j \\rVert$.\n接下来, 我们取$\\lambda =100$, 便可以用上文的Sinkhorn-Knopp算法计算最优转移矩阵$\\mathbf{P}^{*}_\\lambda$, 我们将其可视化如下:\n图中$X_1$中的每一个点都被soft mapping到了$X_2$中, 即$(\\mathbf{P}^{*}_\\lambda)_{ij}$可以被理解为是$X_1$中第$i$个样本和$X_2$中第$j$个样本之间的相似关系, 则接下来我们通过插值:\n$$ x’_i = \\alpha x_i + (1-\\alpha)\\times\\sum\\limits_{j=1}^{m}(\\mathbf{P}^{*}_\\lambda)_{ij}x_j $$\n便可以得到将$X_1$的分布向$X_2$靠近后的结果, 其中$\\alpha$控制了靠近的程度. 注意上式中的$\\mathbf{P}^{*}_\\lambda$需要逐行做归一化, 即需要保证每一行求和为$1$. 插值的结果如下:\n因此Optimal Transport为这种Domain Adaptation问题提供了一种通用的解决方法:\n将分布不同的数据集$X_1$和数据集$X_2$转化为一个最优输运问题. 通过最小化$X_1$和$X_2$之间的Wasserstein或Sinkhorn distance 求得输运矩阵 $\\mathbf{P}^{*}$. 借助输运矩阵$\\mathbf{P}^{*}$通过插值得到$X_1’$, 并在$X_1’$上训练模型, 从而能够泛化到$X_2$上. Optimal Transport还可以用来求解半监督问题, 例如在半监督分类问题中, 我们有少量标注数据, 和大量无标注数据:\n我们同样可以利用Optimal Transport, 计算最优输运矩阵$\\mathbf{P}^{*}$, 从而将无标注样本点soft mapping到有标注样本点上, 之后借助这个mapping去计算soft label或hard label (one-hot).\nColor Transfering 另一个使用Optimal Transfer来解决的直观例子是颜色迁移/风格, 不过这和近些年比较火的Deep Transfer还是不太一样的, Optimal Transfer相对来说更加朴素但是简单. 颜色迁移的任务很简单, 左边是两张给定的原图$X_1$和$X_2$, 希望转换为右边的$X_1’$和$X_2’$, 从而使得$X_1’$和$X_2$相似; $X_2’$和$X_1$相似.\n我们首先需要意识到, 图片由像素构成, 一个像素不过是一个三维向量 (R,G,B), 因此一张图片就可以被理解为一个数据集 (一个分布). 在理解了这一点后, 使用Optimal Transport来实现颜色迁移就相对简单了. 我们就只描述$X_2\\rightarrow X_2’$的过程.\n首先计算从$X_2$到$X_1$的Optimal Transport问题, 我们选择Sinkhorn distance, 通过Sinkhorn-Knopp算法求得$\\mathbf{P}^{*}_\\lambda$. 通过$\\mathbf{P}^{*}_\\lambda$我们插值得到$X_2’$, 换言之, 对于$X_2$中的每一个样本点$x_j$, 我们有其对应的样本点$x_j’$. 训练一个模型$f: X_2\\mapsto X_2’$, 并取$f(X_2)$作为最终结果. 为什么这里我们需要最后第三部训练模型$f$, 而不是直接取第二部得到的$X_2’$作为最终结果呢, 主要原因有两点:\n出于训练效率原因, 实际实现时我们并不是取了$X_2$的所有像素点做训练, 而是只取了一部分, 因此最终我们需要借助模型$f$的泛化能力来将未被选中的像素点映射到$X_2’$中. 最终模型我们选取的是KNeighborsRegressor, 它在对$x_j$做预测时, 并不是只考虑$x_j’$. 还会考虑$x_j$的$K$个neighbor的取值, 因此我们最终得到的$f(X_2)$相对于$X_2’$来说色彩会更加平滑. 2.2. 衡量分布间的距离 此外, Optimal Transport还可以被用来作为衡量两个分布之间的距离的工具. 当然单纯衡量分布之间距离的方式有很多, 例如KL-Divergence, TV-Divergence. 但是Optimal Transport的好处在于, 我们可以通过代价矩阵$\\mathbf{M}$来引入先验知识.\n例如如果想比较两道菜之间的差距, 这可能是十分困难的. 例如红烧牛肉和清蒸鲈鱼之间的差异是多少呢? 但是Optimal Transport提供了一个直觉上的思路.\n首先我们将两道菜打散成菜谱, 例如$\\text{红烧牛肉} = \\begin{bmatrix} 牛肉\\\\酱油\\\\\\vdots\\\\八角\\end{bmatrix}$, $\\text{清蒸鲈鱼}=\\begin{bmatrix} 鲈鱼\\\\\\vdots\\\\耗油\\end{bmatrix}$, 相对来说度量菜谱里的每一个原材料之间的距离更为简单, 我们便可以得到代价矩阵$\\mathbf{M}$. 之后如果我们还知道每道菜里原材料之间的相对比例, 我们还可以将其嵌入到$\\mathbf{r}$和$\\mathbf{c}$中. 因此, 在嵌入了如此多的知识之后, 我们有理由相信通过Optimal Transport求得的距离是更加合理的.\n3. 参考 https://michielstock.github.io/posts/2017/2017-11-5-OptimalTransport/#notes_on_optimal_transport 原始代码: https://github.com/MichielStock/Teaching/tree/master/Optimal_transport 我整理后的代码 (一个jupyter notebook文件): https://github.com/coderlemon17/LemonScripts/tree/master/OptimalTransport ","wordCount":"5891","inLanguage":"en","datePublished":"2022-07-16T14:26:33+08:00","dateModified":"2022-07-16T14:26:33+08:00","author":{"@type":"Person","name":"Xinning Zhou"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://coderlemon17.github.io/posts/2022/07-16-ot/"},"publisher":{"@type":"Organization","name":"Lemon's Blog","logo":{"@type":"ImageObject","url":"https://coderlemon17.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://coderlemon17.github.io/ accesskey=h title="Lemon's Blog (Alt + H)">Lemon's Blog</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://coderlemon17.github.io/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://coderlemon17.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://ml.cs.tsinghua.edu.cn/ title=TSAIL><span>TSAIL</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://coderlemon17.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://coderlemon17.github.io/posts/>Posts</a></div><h1 class=post-title>Optimal Transport入门简述</h1><div class=post-meta><span title='2022-07-16 14:26:33 +0800 +0800'>July 16, 2022</span>&nbsp;·&nbsp;12 min&nbsp;·&nbsp;Xinning Zhou&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/posts/2022/07-16-OT/index.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#0-main-takeaway aria-label="0. Main Takeaway">0. Main Takeaway</a></li><li><a href=#1-%e4%bb%8e%e4%b8%80%e6%ac%a1%e8%81%9a%e4%bc%9a%e8%ae%b2%e8%b5%b7 aria-label="1. 从一次聚会讲起">1. 从一次聚会讲起</a><ul><li><a href=#11-%e6%9c%80%e4%bc%98%e8%be%93%e8%bf%90-optimal-transport aria-label="1.1. 最优输运 (Optimal Transport)">1.1. 最优输运 (Optimal Transport)</a></li><li><a href=#sinkhorn-distance%e5%92%8csinkhorn-knopp%e7%ae%97%e6%b3%95 aria-label="Sinkhorn distance和Sinkhorn-Knopp算法">Sinkhorn distance和Sinkhorn-Knopp算法</a></li></ul></li><li><a href=#2-%e6%9c%80%e4%bc%98%e8%be%93%e8%bf%90%e7%9a%84%e5%ba%94%e7%94%a8%e5%ae%9e%e4%be%8b%e5%90%ab%e4%bb%a3%e7%a0%81 aria-label="2. 最优输运的应用实例(含代码)">2. 最优输运的应用实例(含代码)</a><ul><li><a href=#21-%e5%88%86%e5%b8%83%e5%8c%b9%e9%85%8d aria-label="2.1. 分布匹配">2.1. 分布匹配</a><ul><li><a href=#domain-adaptation aria-label="Domain Adaptation">Domain Adaptation</a></li><li><a href=#color-transfering aria-label="Color Transfering">Color Transfering</a></li></ul></li><li><a href=#22-%e8%a1%a1%e9%87%8f%e5%88%86%e5%b8%83%e9%97%b4%e7%9a%84%e8%b7%9d%e7%a6%bb aria-label="2.2. 衡量分布间的距离">2.2. 衡量分布间的距离</a></li></ul></li><li><a href=#3-%e5%8f%82%e8%80%83 aria-label="3. 参考">3. 参考</a></li></ul></div></details></div><div class=post-content><h1 id=0-main-takeaway>0. Main Takeaway<a hidden class=anchor aria-hidden=true href=#0-main-takeaway>#</a></h1><p>本文主要是对<a href=https://michielstock.github.io/posts/2017/2017-11-5-OptimalTransport/#notes_on_optimal_transport>Notes on Optimal Transport</a> 的翻译及整理, 同时重构了原文提供的代码, 整理到了一个Jupyter Notebook中并增加了注释, 方便理解 (见<a href=https://github.com/coderlemon17/LemonScripts/tree/master/OptimalTransport>我的github</a>).</p><p>本文看完可以了解到</p><ul><li>Sec 1.: 什么是最优输运; <em>Wasserstein metric</em>; <em>Sinkhorn distance</em>; Sinkhorn-Knopp算法</li><li>Sec 2.: 最优输运的实际运用(含代码), 包括训练集和测试集分布不一致时如何插值; 风格迁移.</li><li>Sec 3.: 提供了整理后的代码.</li></ul><h1 id=1-从一次聚会讲起>1. 从一次聚会讲起<a hidden class=anchor aria-hidden=true href=#1-从一次聚会讲起>#</a></h1><p>It&rsquo;s Party Time! 假设实验室要举办一场聚会, 一共有<code>n=8</code>个人, 有<code>m=5</code>种小吃. 我们用$\mathbf{r} = (3,3,4,2,2,2,1)^{\top}\in \mathbb{R}^n$来代表每个人的食量, 用$\mathbf{c}=(4,2,6,4,4)^{\top}\in \mathbb{R}^m$来代表每个小吃的份数, 如下图:</p><p><img src=assets/1-20220707181257-p3aavzl.png#center alt=1.png></p><p><img src=assets/2-20220707181316-ynsz06r.png#center alt=2.png></p><p>同时, 为了让每个人都吃到自己最喜欢的小吃, 我们调查了每个人对每种小吃的喜爱程度, 如下表, $+2$表示最喜欢, $-2$表示最讨厌.</p><div class=data-table><div style=text-align:center;margin-top:1%;margin-bottom:4%><table class="%!s(<nil>)" style=display:inline><thead><tr><th style=text-align:center></th><th style=text-align:center>merveilleux</th><th style=text-align:center>eclair</th><th style=text-align:center>chocolate mousse</th><th style=text-align:center>bavarois</th><th style=text-align:center>carrot cake</th></tr></thead><tbody><tr><td style=text-align:center>Bernard</td><td style=text-align:center>2</td><td style=text-align:center>2</td><td style=text-align:center>1</td><td style=text-align:center>0</td><td style=text-align:center>0</td></tr><tr><td style=text-align:center>Jan</td><td style=text-align:center>0</td><td style=text-align:center>-2</td><td style=text-align:center>-2</td><td style=text-align:center>-2</td><td style=text-align:center>2</td></tr><tr><td style=text-align:center>Willem</td><td style=text-align:center>1</td><td style=text-align:center>2</td><td style=text-align:center>2</td><td style=text-align:center>2</td><td style=text-align:center>-1</td></tr><tr><td style=text-align:center>Hilde</td><td style=text-align:center>2</td><td style=text-align:center>1</td><td style=text-align:center>0</td><td style=text-align:center>1</td><td style=text-align:center>-1</td></tr><tr><td style=text-align:center>Steffie</td><td style=text-align:center>0.5</td><td style=text-align:center>2</td><td style=text-align:center>2</td><td style=text-align:center>1</td><td style=text-align:center>0</td></tr><tr><td style=text-align:center>Marlies</td><td style=text-align:center>0</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>-1</td></tr><tr><td style=text-align:center>Tim</td><td style=text-align:center>-2</td><td style=text-align:center>2</td><td style=text-align:center>2</td><td style=text-align:center>1</td><td style=text-align:center>1</td></tr><tr><td style=text-align:center>Wouter</td><td style=text-align:center>2</td><td style=text-align:center>1</td><td style=text-align:center>2</td><td style=text-align:center>1</td><td style=text-align:center>-1</td></tr></tbody></table></div></div><p>接下来问题来了, 我们要如何分配我们的小吃, 从而保证在<strong>所有人都吃饱, 且所有小吃都被分完</strong>的前提下, 每个人都尽可能吃到自己最喜欢吃的小吃呢?</p><h2 id=11-最优输运-optimal-transport>1.1. 最优输运 (Optimal Transport)<a hidden class=anchor aria-hidden=true href=#11-最优输运-optimal-transport>#</a></h2><p>回顾前文, 我们用$\mathbf{r} = (3,3,4,2,2,2,1)^{\top}\in \mathbb{R}^n$来代表每个人的食量, 用$\mathbf{c}=(4,2,6,4,4)^{\top}\in \mathbb{R}^m$来代表每个小吃的份数(我们假设份数是可被连续分割的). 则我们的目标是求解一个维度为$n\times m$的矩阵$\mathbf{P}$, 其中$\mathbf{P}_{ij}$代表第$i$个人应该被分到几份第$j$个小吃.</p><p>我们用$U(\mathbf{r}, \mathbf{c})$用来表示所有可能的解空间 (即$\mathbf{P}$可能的取值空间):</p><p>$$
U(\mathbf{r}, \mathbf{c}) = \{ \mathbf{P} \in \mathbb{R}^{n\times m}_{\geq 0}\mid \mathbf{P}\mathbf{1}_m = \mathbf{r}, \mathbf{P}^{\top} \mathbf{1}_n =\mathbf{c}\}.
$$</p><p>出于惯例, 我们把每个人对小吃的喜好矩阵取反后得到厌恶/代价矩阵 (cost matrix), 并记为$\mathbf{M}\in \mathbb{R}^{n\times m}$, 则我们的优化任务为:</p><p>$$
d_\mathbf{M}(\mathbf{r}, \mathbf{c}) = \min\limits_{\mathbf{P}\in U(\mathbf{r}, \mathbf{c})}\sum\limits_{ij}\mathbf{P}_{ij}\mathbf{M}_{ij}
$$</p><p>这便被称之为$\mathbf{r}$和$\mathbf{c}$之间的<strong>最优输运问题.</strong> 注意目标函数和限制函数都是线性的, 所以这个最优输运问题可以被线性规划有效求解 (例如单纯形法, 对偶单纯形法等).其中该问题的最优解$d_{\mathbf{M}}(\mathbf{r}, \mathbf{c})$也被称之为<em>Wasserstein distance</em>, 当$\mathbf{r}$, $\mathbf{c}$都是概率分布时, <em>Wasserstein distance</em>便是这两个分布之间的距离的一种度量 (常见的度量还有KL-Divergence, TV-Divergence等).</p><p>Optimal Transport有时候也被称为<em>earth mover distance</em>, 因为当你把$\mathbf{r}$, $\mathbf{c}$想象成两座小沙丘时, $\mathbf{P}_{ij}^{*}$其实描述了你要怎么搬运沙子, 从而以最小的代价让$\mathbf{r}$变成$\mathbf{c}$. 举例</p><div class=data-table><div style=text-align:center;margin-top:1%;margin-bottom:4%><table class="%!s(<nil>)" style=display:inline><thead><tr><th style=text-align:center></th><th style=text-align:center>0.2</th><th style=text-align:center>0.6</th><th style=text-align:center>0.2</th></tr></thead><tbody><tr><td style=text-align:center><strong>0.6</strong></td><td style=text-align:center>0</td><td style=text-align:center>0.6</td><td style=text-align:center>0</td></tr><tr><td style=text-align:center><strong>0.4</strong></td><td style=text-align:center>0.2</td><td style=text-align:center>0</td><td style=text-align:center>0.2</td></tr></tbody></table></div></div><p>假设$\mathbf{r} = (0.6, 0.4)^{\top}$, $\mathbf{c} = (0.2, 0.6, 0.2)^{\top}$, 我们先不考虑代价矩阵$\mathbf{M}$. 上表就描述了一种搬运方式: 即把$\mathbf{r}$的第一堆沙子全搬到$\mathbf{c}$的第二堆, 把$\mathbf{r}$的第二堆沙子平均搬到$\mathbf{c}$的第一堆和第三堆.</p><h2 id=sinkhorn-distance和sinkhorn-knopp算法>Sinkhorn distance和Sinkhorn-Knopp算法<a hidden class=anchor aria-hidden=true href=#sinkhorn-distance和sinkhorn-knopp算法>#</a></h2><p>对于一般的最优输运问题, 我们可以使用线性规划求解, 但是当$n$和$m$较大时, 求解会十分耗时, 于是我们不妨考虑一个"更难"的加了正则项问题:</p><p>$$
d^{\lambda}_\mathbf{M}(\mathbf{r}, \mathbf{c}) = \min\limits_{\mathbf{P}\in U(\mathbf{r}, \mathbf{c})}\sum\limits_{ij}\mathbf{P}_{ij}\mathbf{M}_{ij} - \frac{1}{\lambda} h(\mathbf{P}), \lambda > 0.
$$</p><p>这里$h(\mathbf{P})$是矩阵$\mathbf{P}$的信息熵矩阵 (可以先normalize $\mathbf{P}$再求):</p><p>$$
h(\mathbf{P}) = -\sum\limits_{ij} \mathbf{P}_{ij}\log \mathbf{P}_{ij}
$$</p><p>这里引入的正则项$h(\mathbf{P})$越大, 表示分配矩阵$\mathbf{P}$越均匀: 即每个人会越均匀的分配到每个小吃, 而$\lambda$作为超参, 用来平衡求解最优输运和让分配矩阵$\mathbf{P}$越均匀.</p><p>若我们记$\mathbf{P}^{*} _{\lambda}$为上述问题对应的最优解, 则$\sum\limits_{ij}(\mathbf{P}^{*}_{\lambda})_{ij}\mathbf{M}_{ij}$被称为<em>Sinkhorn distance</em>. (注意: 正则项$h(\mathbf{P})$的值是不被计入Sinkhorn distance的.) 在一些问题中, 使用<em>Sinkhorn distance</em>得到的结果会比<em>Wasserstein distance</em>要更好, 因为引入$h(\mathbf{P})$相当于引入了一个<strong>先验</strong>: 即在不考虑代价矩阵的情况下, 我们希望每个人分到的越均匀越好.</p><hr><p>虽然一般来说, 加入正则项后会让问题的求解更复杂, 但是对于<em>Sinkhorn distance</em>则是一个例外, 事实上, 我们有一个非常高效且简洁的算法用来求解<em>Sinkhorn distance</em>, 即Sinkhorn-Knopp算法.</p><p>该算法的正确性我们这里不做展开, 感兴趣的同学可以自行查阅<a href=https://www.cerfacs.fr/algor/reports/2006/TR_PA_06_42.pdf>《the sinkhorn-knopp algorithm: convergence and applications》</a>.</p><p>Sinkhorn-Knopp的算法基于一个发现, $d^{\lambda}_\mathbf{M}(\mathbf{r}, \mathbf{c})$的最优解$\mathbf{P}^{*} _{\lambda}$一定满足如下形式:</p><p>$$
\left(\mathbf{P}_{\lambda}^{\star}\right)_{i j}=\alpha_{i} \beta_{j} e^{-\lambda \mathbf{M}_{i j}}
$$</p><p>其中$\alpha_i$, $\beta_j$是待求解的常数, 从而保证$\mathbf{P}_{\lambda}^{\star}$横向求和是$\mathbf{r}$, 纵向求和是$\mathbf{c}$. 因此我们采用一种迭代的思想去求解, 即</p><p><strong>Input:</strong> $\mathbf{M}, \mathbf{r}, \mathbf{c}, \lambda$</p><p><strong>Initialize:</strong> $(\mathbf{P}_\lambda)_{ij} = e^{-\lambda \mathbf{M}_{ij}}$</p><p><strong>Loop:</strong></p><blockquote><ol><li><strong>Scale每一行,</strong> 从而横向求和是$\mathbf{r}$.</li><li><strong>Scale每一列,</strong> 从而纵向求和是$\mathbf{c}$.</li></ol></blockquote><p><strong>Until</strong> convergence.</p><hr><p>下面给出对应的Python实现:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>compute</span>\_optimal\_transport(M, r, c, lam, epsilon<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-5</span>):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Computes the optimal transport matrix and Slinkhorn distance using the
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Sinkhorn-Knopp algorithm
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Inputs:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        - M : cost matrix (n x m)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        - r : vector of marginals (n, )
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        - c : vector of marginals (m, )
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        - lam : strength of the entropic regularization
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        - epsilon : convergence parameter
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Output:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        - P : optimal transport matrix (n x m)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        - dist : Sinkhorn distance
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    n, m <span style=color:#f92672>=</span> M<span style=color:#f92672>.</span>shape
</span></span><span style=display:flex><span>    P <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>exp(<span style=color:#f92672>-</span> lam <span style=color:#f92672>*</span> M)
</span></span><span style=display:flex><span>    <span style=color:#75715e># Avoiding poor math condition</span>
</span></span><span style=display:flex><span>    P <span style=color:#f92672>/=</span> P<span style=color:#f92672>.</span>sum()
</span></span><span style=display:flex><span>    u <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros(n)
</span></span><span style=display:flex><span>    <span style=color:#75715e># Normalize this matrix so that P.sum(1) == r, P.sum(0) == c</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>while</span> np<span style=color:#f92672>.</span>max(np<span style=color:#f92672>.</span>abs(u <span style=color:#f92672>-</span> P<span style=color:#f92672>.</span>sum(<span style=color:#ae81ff>1</span>))) <span style=color:#f92672>&gt;</span> epsilon:
</span></span><span style=display:flex><span>        <span style=color:#75715e># Shape (n, )</span>
</span></span><span style=display:flex><span>        u <span style=color:#f92672>=</span> P<span style=color:#f92672>.</span>sum(<span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        P <span style=color:#f92672>*=</span> (r <span style=color:#f92672>/</span> u)<span style=color:#f92672>.</span>reshape((<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>        P <span style=color:#f92672>*=</span> (c <span style=color:#f92672>/</span> P<span style=color:#f92672>.</span>sum(<span style=color:#ae81ff>0</span>))<span style=color:#f92672>.</span>reshape((<span style=color:#ae81ff>1</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> P, np<span style=color:#f92672>.</span>sum(P <span style=color:#f92672>*</span> M)
</span></span></code></pre></div><p>借助Sinkhorn-Knopp算法, 我们可以求解上述加入了entropy正则项的派对问题, 结果如下:</p><p><img src=assets/1-20220715181334-eheubp5.png#center alt=1.png></p><p><img src=assets/2-20220715181348-qz4ogh9.png#center alt=2.png></p><p>可以注意到, 当$\lambda$的值越小, 我们加入的惩罚项$\frac{1}{\lambda} h(\mathbf{P})$会使得最终解更偏向homogeneous的分布, 即每个人分到的小吃会更加均匀, 这也意味着每个人可能会被更多地分到自己不喜欢吃的小吃.</p><p>事实上, 最优输运有着一个很优雅的几何解释:<figure class=align-center><img loading=lazy src=assets/image-20220715181620-31zikh4.png#center width=30%></figure></p><p>上图中, $U(\mathbf{r}, \mathbf{c})$包含了所有可行解, 而代价矩阵$\mathbf{M}$则给定了一个方向, 从而指明了什么样的解$\mathbf{P}$是更优的. 在没有正则项$\frac{1}{\lambda} h(\mathbf{P})$的情况下, 最优输运问题的解会在$U(\mathbf{r}, \mathbf{c})$的一个极点取到 (别忘了最优输运本质上是一个线性规划问题). 而在加入正则项$\frac{1}{\lambda} h(\mathbf{P})$后, 我们希望得到的是一个更smooth的解, 例如图中红色椭圆与黄线相切的位置. 这也就解释了为什么加入正则项后求解会更加容易, 因为我们可以不用再管$U(\mathbf{r}, \mathbf{c})$复杂的边界了.</p><p>在极端情况下, 当$\lambda \rightarrow \infty$, $\mathbf{P}_\lambda^{*}$会无限接近$\mathbf{P}^{*}$ (直到出现数值不稳定性). 而当$\lambda \rightarrow 0$的时候, 我们只会考虑最大化entropy项$h(\mathbf{P})$, 从而$\mathbf{P}^{*}_{0} =\mathbf{r}^{\top}\mathbf{c}$. (这是满足约束条件的可行解中entropy最大的解.)</p><h1 id=2-最优输运的应用实例含代码>2. 最优输运的应用实例(含代码)<a hidden class=anchor aria-hidden=true href=#2-最优输运的应用实例含代码>#</a></h1><p>在了解了最优输运的原理后, 我们给出几个实例: 包括分布匹配, 风格迁移. 代码即注释见<a href=https://github.com/coderlemon17/LemonScripts/tree/master/OptimalTransport>github</a>.</p><h2 id=21-分布匹配>2.1. 分布匹配<a hidden class=anchor aria-hidden=true href=#21-分布匹配>#</a></h2><p>分布匹配所关心的问题是, 当我们有两个不同的分布$p_1(X)$和$p_2(X)$, 我们要如何修改分布$p_1$使得其和$p_2$尽可能接近. 这听起来可能有些抽象, 我们不妨考虑一个很具体的例子: 假设我们要做一个目标检测的任务, 但是我们训练集全都是白天的图像, 而测试集全都是黑夜的图像, 那我们要如何调整训练集, 从而使得我们的模型最终能泛化到黑夜上呢?</p><blockquote><p><code>Note</code>:</p><p>假设我们有数据集$\left \{ x_i\right \}_{i=1,\cdots, N}$, 则它其实就定义了一个分布, 一般称为经验分布, 该分布的概率密度函数为:</p><p>$$
p(X=x) = \frac{1}{N}\sum_i^N \mathbf{1}_{x_i = x}
$$</p><p>所以两个数据集之间的差异就可以被转化为两个分布间的差异问题.</p></blockquote><h3 id=domain-adaptation>Domain Adaptation<a hidden class=anchor aria-hidden=true href=#domain-adaptation>#</a></h3><p>假设我们有两个数据集, 分别为$X_1$ (蓝色) 和 $X_2$ (橘色), 其中$X_1$有$n$个点, $X_2$有$m$个点. 我们不妨就认为$X_1$是白天的图像, $X_2$是夜晚的图像.</p><p><img src=assets/1-20220715184036-xv0xk3j.png#center alt=1.png></p><p>注意这里$\mathbf{r}$和$\mathbf{c}$都是均匀分布 ( $p(X=x) = \frac{1}{N}\sum\limits_i^N \mathbf{1}_{x_i = x}$, 我们假设图片之间两两不同 ). 对于代价矩阵$\mathbf{M}$, $\mathbf{M}_{ij}$代表了把图$i$变成图$j$的代价, 这里我们就取Euclidean Distance, 即$\lVert x_i -x_j \rVert$.</p><p>接下来, 我们取$\lambda =100$, 便可以用上文的Sinkhorn-Knopp算法计算最优转移矩阵$\mathbf{P}^{*}_\lambda$, 我们将其可视化如下:</p><p><img src=assets/1-20220716113733-t9vvbpd.png#center alt=1.png></p><p>图中$X_1$中的每一个点都被soft mapping到了$X_2$中, 即$(\mathbf{P}^{*}_\lambda)_{ij}$可以被理解为是$X_1$中第$i$个样本和$X_2$中第$j$个样本之间的相似关系, 则接下来我们通过插值:</p><p>$$
x&rsquo;_i = \alpha x_i + (1-\alpha)\times\sum\limits_{j=1}^{m}(\mathbf{P}^{*}_\lambda)_{ij}x_j
$$</p><p>便可以得到将$X_1$的分布向$X_2$靠近后的结果, 其中$\alpha$控制了靠近的程度. 注意上式中的$\mathbf{P}^{*}_\lambda$需要逐行做归一化, 即需要保证每一行求和为$1$. 插值的结果如下:</p><p><img src=assets/1-20220716114635-2qoszq6.png#center alt=1.png></p><p>因此Optimal Transport为这种Domain Adaptation问题提供了一种通用的解决方法:</p><ol><li>将分布不同的数据集$X_1$和数据集$X_2$转化为一个最优输运问题.</li><li>通过最小化$X_1$和$X_2$之间的<em>Wasserstein</em>或<em>Sinkhorn distance</em> 求得输运矩阵 $\mathbf{P}^{*}$.</li><li>借助输运矩阵$\mathbf{P}^{*}$通过插值得到$X_1&rsquo;$, 并在$X_1&rsquo;$上训练模型, 从而能够泛化到$X_2$上.</li></ol><p>Optimal Transport还可以用来求解半监督问题, 例如在半监督分类问题中, 我们有少量标注数据, 和大量无标注数据:</p><p><img src=assets/1-20220716115331-ur2z8fq.png#center alt=1.png></p><p>我们同样可以利用Optimal Transport, 计算最优输运矩阵$\mathbf{P}^{*}$, 从而将无标注样本点soft mapping到有标注样本点上, 之后借助这个mapping去计算soft label或hard label (one-hot).</p><p><img src=assets/1-20220716115705-jt42nbs.png#center alt=1.png></p><h3 id=color-transfering>Color Transfering<a hidden class=anchor aria-hidden=true href=#color-transfering>#</a></h3><p>另一个使用Optimal Transfer来解决的直观例子是颜色迁移/风格, 不过这和近些年比较火的Deep Transfer还是不太一样的, Optimal Transfer相对来说更加朴素但是简单. 颜色迁移的任务很简单, 左边是两张给定的原图$X_1$和$X_2$, 希望转换为右边的$X_1&rsquo;$和$X_2&rsquo;$, 从而使得$X_1&rsquo;$和$X_2$相似; $X_2&rsquo;$和$X_1$相似.</p><p><img src=assets/color_transfer-20220716121607-2we28t4.png#center alt=color_transfer.png></p><p>我们首先需要意识到, 图片由<strong>像素</strong>构成, 一个像素不过是一个三维向量 (R,G,B), 因此一张图片就可以被理解为一个数据集 (一个分布). 在理解了这一点后, 使用Optimal Transport来实现颜色迁移就相对简单了. 我们就只描述$X_2\rightarrow X_2&rsquo;$的过程.</p><ol><li>首先计算从$X_2$到$X_1$的Optimal Transport问题, 我们选择<em>Sinkhorn distance</em>, 通过Sinkhorn-Knopp算法求得$\mathbf{P}^{*}_\lambda$.</li><li>通过$\mathbf{P}^{*}_\lambda$我们插值得到$X_2&rsquo;$, 换言之, 对于$X_2$中的每一个样本点$x_j$, 我们有其对应的样本点$x_j&rsquo;$.</li><li>训练一个模型$f: X_2\mapsto X_2&rsquo;$, 并取$f(X_2)$作为最终结果.</li></ol><p>为什么这里我们需要最后第三部训练模型$f$, 而不是直接取第二部得到的$X_2&rsquo;$作为最终结果呢, 主要原因有两点:</p><ol><li>出于训练效率原因, 实际实现时我们并不是取了$X_2$的所有像素点做训练, 而是只取了一部分, 因此最终我们需要借助模型$f$的泛化能力来将未被选中的像素点映射到$X_2&rsquo;$中.</li><li>最终模型我们选取的是<a href=http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html>KNeighborsRegressor</a>, 它在对$x_j$做预测时, 并不是只考虑$x_j&rsquo;$. 还会考虑$x_j$的$K$个neighbor的取值, 因此我们最终得到的$f(X_2)$相对于$X_2&rsquo;$来说色彩会更加平滑.</li></ol><h2 id=22-衡量分布间的距离>2.2. 衡量分布间的距离<a hidden class=anchor aria-hidden=true href=#22-衡量分布间的距离>#</a></h2><p>此外, Optimal Transport还可以被用来作为衡量两个分布之间的距离的工具. 当然单纯衡量分布之间距离的方式有很多, 例如KL-Divergence, TV-Divergence. 但是Optimal Transport的好处在于, 我们可以通过代价矩阵$\mathbf{M}$来引入<strong>先验知识</strong>.</p><p>例如如果想比较两道菜之间的差距, 这可能是十分困难的. 例如红烧牛肉和清蒸鲈鱼之间的差异是多少呢? 但是Optimal Transport提供了一个直觉上的思路.</p><p>首先我们将两道菜打散成菜谱, 例如$\text{红烧牛肉} = \begin{bmatrix} 牛肉\\酱油\\\vdots\\八角\end{bmatrix}$, $\text{清蒸鲈鱼}=\begin{bmatrix} 鲈鱼\\\vdots\\耗油\end{bmatrix}$, 相对来说度量菜谱里的每一个原材料之间的距离更为简单, 我们便可以得到代价矩阵$\mathbf{M}$. 之后如果我们还知道每道菜里原材料之间的相对比例, 我们还可以将其嵌入到$\mathbf{r}$和$\mathbf{c}$中. 因此, 在嵌入了如此多的知识之后, 我们有理由相信通过Optimal Transport求得的距离是更加合理的.</p><h1 id=3-参考>3. 参考<a hidden class=anchor aria-hidden=true href=#3-参考>#</a></h1><ul><li><a href=https://michielstock.github.io/posts/2017/2017-11-5-OptimalTransport/#notes_on_optimal_transport>https://michielstock.github.io/posts/2017/2017-11-5-OptimalTransport/#notes_on_optimal_transport</a></li><li>原始代码: <a href=https://github.com/MichielStock/Teaching/tree/master/Optimal_transport>https://github.com/MichielStock/Teaching/tree/master/Optimal_transport</a></li><li>我整理后的代码 (一个jupyter notebook文件): <a href=https://github.com/coderlemon17/LemonScripts/tree/master/OptimalTransport>https://github.com/coderlemon17/LemonScripts/tree/master/OptimalTransport</a></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://coderlemon17.github.io/tags/optimal-transport/>Optimal Transport</a></li></ul><nav class=paginav><a class=next href=https://coderlemon17.github.io/posts/2022/05-11-mcmc/><span class=title>Next »</span><br><span>详解Markov Chain Monte Carlo (MCMC): 从拒绝-接受采样到Gibbs Sampling</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Optimal Transport入门简述 on twitter" href="https://twitter.com/intent/tweet/?text=Optimal%20Transport%e5%85%a5%e9%97%a8%e7%ae%80%e8%bf%b0&url=https%3a%2f%2fcoderlemon17.github.io%2fposts%2f2022%2f07-16-ot%2f&hashtags=OptimalTransport"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Optimal Transport入门简述 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fcoderlemon17.github.io%2fposts%2f2022%2f07-16-ot%2f&title=Optimal%20Transport%e5%85%a5%e9%97%a8%e7%ae%80%e8%bf%b0&summary=Optimal%20Transport%e5%85%a5%e9%97%a8%e7%ae%80%e8%bf%b0&source=https%3a%2f%2fcoderlemon17.github.io%2fposts%2f2022%2f07-16-ot%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Optimal Transport入门简述 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fcoderlemon17.github.io%2fposts%2f2022%2f07-16-ot%2f&title=Optimal%20Transport%e5%85%a5%e9%97%a8%e7%ae%80%e8%bf%b0"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Optimal Transport入门简述 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fcoderlemon17.github.io%2fposts%2f2022%2f07-16-ot%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Optimal Transport入门简述 on whatsapp" href="https://api.whatsapp.com/send?text=Optimal%20Transport%e5%85%a5%e9%97%a8%e7%ae%80%e8%bf%b0%20-%20https%3a%2f%2fcoderlemon17.github.io%2fposts%2f2022%2f07-16-ot%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Optimal Transport入门简述 on telegram" href="https://telegram.me/share/url?text=Optimal%20Transport%e5%85%a5%e9%97%a8%e7%ae%80%e8%bf%b0&url=https%3a%2f%2fcoderlemon17.github.io%2fposts%2f2022%2f07-16-ot%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://coderlemon17.github.io/>Lemon's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>