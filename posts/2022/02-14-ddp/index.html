<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>torch.nn.parallel.DistributedDataParallel: 快速上手 | Lemon's Blog</title><meta name=keywords content="torch"><meta name=description content="0. 前言 Node: 一个节点, 可以理解为一台电脑.
Device: 工作设备, 可以简单理解为一张卡, 即一个GPU.
Process: 一个进程, 可以简单理解为一个Python程序.
Threading: 一个线程, 一个进程可以有多个线程, 它们共享资源.
  建议: 使用torchrun, 不要使用multiprocessing, torchrun会在程序中断后帮你杀死线程, 使用multiprocessing很容易造成僵尸线程, 资源无法释放.  1. 什么是数据并行化 随着模型参数和数据量越来越大, 分布式训练成为了深度学习模型训练中越来越重要的一环. 分布式训练包括两类: 模型并行化 和 数据并行化. 在模型并行化中, 一个Device负责处理模型的一个切片 (例如模型的一层); 而在数据并行化中, 一个Device负责处理数据的一个切片 (即Batch的一部分). 我们今天讨论的torch.nn.parallel.DistributedDataParallel就是由pytorch提供的一种数据并行化方式.
2. 为什么要使用torch.nn.parallel.DistributedDataParallel 相较于torch.nn.parallel.DistributedDataParallel, 一个更易于使用也更被人熟知的接口是torch.nn.DataParallel. 该接口只需要一行修改即可实现&#34;数据并行化&#34; (具体参考知乎):
device_ids = [0, 1] model = torch.nn.DataParallel(model, device_ids=device_ids) 此方法虽然简单, 但是存在若干问题, 例如设备间负载不均; 效率不高等. 现在官方推荐的方法为torch.nn.parallel.DistributedDataParallel ( Use nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel ).
为何如此呢? 简单来说, torch."><meta name=author content="Xinning Zhou"><link rel=canonical href=https://coderlemon17.github.io/posts/2022/02-14-ddp/><link crossorigin=anonymous href=/assets/css/stylesheet.min.f62327f3747482455304c712f4fc419bbe4fb8f71c7691c5081f28f3c3a6f8e4.css integrity="sha256-9iMn83R0gkVTBMcS9PxBm75PuPccdpHFCB8o88Om+OQ=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.2840b7fccd34145847db71a290569594bdbdb00047097f75d6495d162f5d7dff.js integrity="sha256-KEC3/M00FFhH23GikFaVlL29sABHCX911kldFi9dff8=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://coderlemon17.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://coderlemon17.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://coderlemon17.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://coderlemon17.github.io/apple-touch-icon.png><link rel=mask-icon href=https://coderlemon17.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.1/es5/tex-mml-chtml.min.js></script>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,o,i,a,t,n,s){e.GoogleAnalyticsObject=t,e[t]=e[t]||function(){(e[t].q=e[t].q||[]).push(arguments)},e[t].l=1*new Date,n=o.createElement(i),s=o.getElementsByTagName(i)[0],n.async=1,n.src=a,s.parentNode.insertBefore(n,s)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="torch.nn.parallel.DistributedDataParallel: 快速上手"><meta property="og:description" content="0. 前言 Node: 一个节点, 可以理解为一台电脑.
Device: 工作设备, 可以简单理解为一张卡, 即一个GPU.
Process: 一个进程, 可以简单理解为一个Python程序.
Threading: 一个线程, 一个进程可以有多个线程, 它们共享资源.
  建议: 使用torchrun, 不要使用multiprocessing, torchrun会在程序中断后帮你杀死线程, 使用multiprocessing很容易造成僵尸线程, 资源无法释放.  1. 什么是数据并行化 随着模型参数和数据量越来越大, 分布式训练成为了深度学习模型训练中越来越重要的一环. 分布式训练包括两类: 模型并行化 和 数据并行化. 在模型并行化中, 一个Device负责处理模型的一个切片 (例如模型的一层); 而在数据并行化中, 一个Device负责处理数据的一个切片 (即Batch的一部分). 我们今天讨论的torch.nn.parallel.DistributedDataParallel就是由pytorch提供的一种数据并行化方式.
2. 为什么要使用torch.nn.parallel.DistributedDataParallel 相较于torch.nn.parallel.DistributedDataParallel, 一个更易于使用也更被人熟知的接口是torch.nn.DataParallel. 该接口只需要一行修改即可实现&#34;数据并行化&#34; (具体参考知乎):
device_ids = [0, 1] model = torch.nn.DataParallel(model, device_ids=device_ids) 此方法虽然简单, 但是存在若干问题, 例如设备间负载不均; 效率不高等. 现在官方推荐的方法为torch.nn.parallel.DistributedDataParallel ( Use nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel ).
为何如此呢? 简单来说, torch."><meta property="og:type" content="article"><meta property="og:url" content="https://coderlemon17.github.io/posts/2022/02-14-ddp/"><meta property="og:image" content="https://coderlemon17.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-02-14T21:32:33+08:00"><meta property="article:modified_time" content="2022-02-14T21:32:33+08:00"><meta property="og:site_name" content="Lemon's Blog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://coderlemon17.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="torch.nn.parallel.DistributedDataParallel: 快速上手"><meta name=twitter:description content="0. 前言 Node: 一个节点, 可以理解为一台电脑.
Device: 工作设备, 可以简单理解为一张卡, 即一个GPU.
Process: 一个进程, 可以简单理解为一个Python程序.
Threading: 一个线程, 一个进程可以有多个线程, 它们共享资源.
  建议: 使用torchrun, 不要使用multiprocessing, torchrun会在程序中断后帮你杀死线程, 使用multiprocessing很容易造成僵尸线程, 资源无法释放.  1. 什么是数据并行化 随着模型参数和数据量越来越大, 分布式训练成为了深度学习模型训练中越来越重要的一环. 分布式训练包括两类: 模型并行化 和 数据并行化. 在模型并行化中, 一个Device负责处理模型的一个切片 (例如模型的一层); 而在数据并行化中, 一个Device负责处理数据的一个切片 (即Batch的一部分). 我们今天讨论的torch.nn.parallel.DistributedDataParallel就是由pytorch提供的一种数据并行化方式.
2. 为什么要使用torch.nn.parallel.DistributedDataParallel 相较于torch.nn.parallel.DistributedDataParallel, 一个更易于使用也更被人熟知的接口是torch.nn.DataParallel. 该接口只需要一行修改即可实现&#34;数据并行化&#34; (具体参考知乎):
device_ids = [0, 1] model = torch.nn.DataParallel(model, device_ids=device_ids) 此方法虽然简单, 但是存在若干问题, 例如设备间负载不均; 效率不高等. 现在官方推荐的方法为torch.nn.parallel.DistributedDataParallel ( Use nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel ).
为何如此呢? 简单来说, torch."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://coderlemon17.github.io/posts/"},{"@type":"ListItem","position":2,"name":"torch.nn.parallel.DistributedDataParallel: 快速上手","item":"https://coderlemon17.github.io/posts/2022/02-14-ddp/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"torch.nn.parallel.DistributedDataParallel: 快速上手","name":"torch.nn.parallel.DistributedDataParallel: 快速上手","description":"0. 前言 Node: 一个节点, 可以理解为一台电脑.\nDevice: 工作设备, 可以简单理解为一张卡, 即一个GPU.\nProcess: 一个进程, 可以简单理解为一个Python程序.\nThreading: 一个线程, 一个进程可以有多个线程, 它们共享资源.\n  建议: 使用torchrun, 不要使用multiprocessing, torchrun会在程序中断后帮你杀死线程, 使用multiprocessing很容易造成僵尸线程, 资源无法释放.  1. 什么是数据并行化 随着模型参数和数据量越来越大, 分布式训练成为了深度学习模型训练中越来越重要的一环. 分布式训练包括两类: 模型并行化 和 数据并行化. 在模型并行化中, 一个Device负责处理模型的一个切片 (例如模型的一层); 而在数据并行化中, 一个Device负责处理数据的一个切片 (即Batch的一部分). 我们今天讨论的torch.nn.parallel.DistributedDataParallel就是由pytorch提供的一种数据并行化方式.\n2. 为什么要使用torch.nn.parallel.DistributedDataParallel 相较于torch.nn.parallel.DistributedDataParallel, 一个更易于使用也更被人熟知的接口是torch.nn.DataParallel. 该接口只需要一行修改即可实现\u0026quot;数据并行化\u0026quot; (具体参考知乎):\ndevice_ids = [0, 1] model = torch.nn.DataParallel(model, device_ids=device_ids) 此方法虽然简单, 但是存在若干问题, 例如设备间负载不均; 效率不高等. 现在官方推荐的方法为torch.nn.parallel.DistributedDataParallel ( Use nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel ).\n为何如此呢? 简单来说, torch.","keywords":["torch"],"articleBody":"0. 前言 Node: 一个节点, 可以理解为一台电脑.\nDevice: 工作设备, 可以简单理解为一张卡, 即一个GPU.\nProcess: 一个进程, 可以简单理解为一个Python程序.\nThreading: 一个线程, 一个进程可以有多个线程, 它们共享资源.\n  建议: 使用torchrun, 不要使用multiprocessing, torchrun会在程序中断后帮你杀死线程, 使用multiprocessing很容易造成僵尸线程, 资源无法释放.  1. 什么是数据并行化 随着模型参数和数据量越来越大, 分布式训练成为了深度学习模型训练中越来越重要的一环. 分布式训练包括两类: 模型并行化 和 数据并行化. 在模型并行化中, 一个Device负责处理模型的一个切片 (例如模型的一层); 而在数据并行化中, 一个Device负责处理数据的一个切片 (即Batch的一部分). 我们今天讨论的torch.nn.parallel.DistributedDataParallel就是由pytorch提供的一种数据并行化方式.\n2. 为什么要使用torch.nn.parallel.DistributedDataParallel 相较于torch.nn.parallel.DistributedDataParallel, 一个更易于使用也更被人熟知的接口是torch.nn.DataParallel. 该接口只需要一行修改即可实现\"数据并行化\" (具体参考知乎):\ndevice_ids = [0, 1] model = torch.nn.DataParallel(model, device_ids=device_ids) 此方法虽然简单, 但是存在若干问题, 例如设备间负载不均; 效率不高等. 现在官方推荐的方法为torch.nn.parallel.DistributedDataParallel ( Use nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel ).\n为何如此呢? 简单来说, torch.nn.DataParallel 实现的是单机-多线程 (Single-Node Multi-threading), 而torch.nn.parallel.DistributedDataParallel 实现的是 单机/多机-多进程 (Single/Multi-Node Multi-process). 即torch.nn.parallel.DistributedDataParallel中的每一个模型是由一个独立的Process来控制的.1\n此外, 官方文档中指出torch.nn.parallel.DistributedDataParallel是和模型并行化兼容的, 而torch.nn.DataParallel则不可以.\n3. torch.nn.parallel.DistributedDataParallel 究竟在做什么? Fig 1. torch.nn.parallel.DistributedDataParallel (https://yangkky.github.io/2019/07/08/distributed-pytorch-tutorial.html)   简单来说, 如果我们有$N$个Device (即$N$张卡), 每次的batch有$N\\times M$个数据, 那我们如果能将模型分别复制到这$N$张卡上, 每张卡负责计算$M$个数据的$loss$的平均梯度, 然后将这些$N$张卡上梯度平均起来2, 同时将梯度更新到所有模型上3. 那我们相当于花了计算$M$个数据的时间, 完成了$N\\times M$条数据的计算.\n而多机希望实现的是: 简单来说, 就是你只需要知道我有多少个Device, 而不需要管这些Device分布在多少个Node上.\n而为了保证模型间参数的同步, 多设备间需要通讯, 这是通过后端 (Backend 来完成的, 见torch.distributed), 简单来说: 如果想使用GPU训练用nccl, 如果使用CPU用gloo4.\n Each process maintains its own optimizer and performs a complete optimization step with each iteration. While this may appear redundant, since the gradients have already been gathered together and averaged across processes and are thus the same for every process, this means that no parameter broadcast step is needed, reducing time spent transferring tensors between nodes. Each process contains an independent Python interpreter, eliminating the extra interpreter overhead and “GIL-thrashing” that comes from driving several execution threads, model replicas, or GPUs from a single Python process. This is especially important for models that make heavy use of the Python runtime, including models with recurrent layers or many small components.  4. torch.nn.parallel.DistributedDataParallel 使用范例 导入需要的库 import os import torch import argparse import torch.distributed as dist import torch.multiprocessing as mp from torch.nn.parallel import DistributedDataParallel as DDP import torch.optim as optim import torch.nn as nn import torch.distributed as dist from datetime import timedelta 定义一个简单的模型 class ToyModel(nn.Module):  def __init__(self):  super(ToyModel, self).__init__()  self.net1 = nn.Linear(10, 10)  self.relu = nn.ReLU()  self.net2 = nn.Linear(10, 5)   def forward(self, x):  return self.net2(self.relu(self.net1(x))) Fig 1. 中的多个Process形成了一个Process group, 在使用torch.nn.parallel.DistributedDataParallel之间我们需要初始化它, 初始化需要两个参数global_rank 和 world_size:\n  其中world_size是指你一共有多少Process, 即world_size = 节点数量 * 每个节点上有多少Process = nnode * nproc_per_node.\n  而对于每一个Process, 它都有一个local_rank和global_rank, local_rank对应的就是该Process在自己的Node上的编号, 而global_rank就是全局的编号.\n 比如你有$2$个Node, 每个Node上各有$2$个Proess (Process0, Process1, Process2, Process3). 那么对于Process2来说, 它的local_rank就是$0$ (即它在Node1上是第$0$个Process), global_rank 就是$2$. 不难发现, local_rank对应的就是该Process需要使用的Device(GPU)编号 (并不一定, 但这是一种方便的方法).    def setup(global_rank, world_size):  # 配置Master Node的信息  # os.environ['MASTER_ADDR'] = 'localhost'  os.environ['MASTER_ADDR'] = 'XXX.XXX.XXX.XXX'  # os.environ['MASTER_PORT'] = '23555'  os.environ['MASTER_PORT'] = 'XXXX'   # 初始化Process Group  # 关于init_method, 参数详见https://pytorch.org/docs/stable/distributed.html#initialization  dist.init_process_group(\"nccl\", init_method='env://', rank=global_rank, world_size=world_size, timeout=timedelta(seconds=5))  def cleanup():  dist.destroy_process_group()  对于Process group来说需要有一个Master node, 可以理解为是这个Process group的根节点, 我们一般配置为为Node0, 在后续在各个节点上启动代码时, 最好也是先从Node0开始启动. 需要确保Master node的IP地址可访问, 且端口没有被占用. 对于响应超时可以自行设置超时时间timeout, 官方建议设置一个较大的时间, 以防可能出现的网络延迟.  定义训练过程 接下来我们定义在每一个Process中我们希望执行的代码.\ndef run_demo(local_rank, args):  # 计算global_rank和world_size  global_rank = local_rank + args.node_rank * args.nproc_per_node  world_size = args.nnode * args.nproc_per_node  setup(global_rank=global_rank, world_size=world_size)  # 设置seed  torch.manual_seed(args.seed)   # 创建模型, 并将其移动到local_rank对应的GPU上  model = ToyModel().to(local_rank)  ddp_model = DDP(model, device_ids=[local_rank], output_device=local_rank)   loss_fn = nn.MSELoss()  optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)   optimizer.zero_grad()  outputs = ddp_model(torch.randn(20, 10))  labels = torch.randn(20, 5).to(local_rank)  loss_fn(outputs, labels).backward()  optimizer.step()   print([data for data in model.parameters()])   cleanup()   需要注意, 我们每一个Process的应该运行在它自己对应的GPU上, 所以我们在代码中需要加上以下三种方式的一种:\n  # 方法1 torch.cuda.set_device(local_rank) # 方法2 with torch.cuda.device(local_rank) # 方法3 model = ToyModel().to(local_rank)     在DDP中, 所有模型都是以相同的参数被初始化, 同时训练过程中的梯度会在backward pass中被同步, 这就保证了在optimizer的优化过程中所有模型的参数保持一致.\n  多线程执行 最后, 我们需要在每一个Node上启动nproc_per_node个Process, 这一步可以使用torch.distributed.launch/torchrun/multiprocessing来实现:\n  虽然为了代码清晰这里我们用了multiprocessing来实现, 但是在代码意外退出的时候它容易出现僵尸进程等Bug (Github), 在工程代码中不建议使用.\n  torchrun是为了替代torch.distributed.launch的新型启动方式, 可以支持ELASTIC LAUNCH, 即动态控制启动的节点数量, 但是由于是新功能, 只有最新的torch 1.10, 处于兼容性考虑还是建议使用torch.distributed.launch.\n  Single-node multi-worker   torchrun  --standalone  --nnodes=1  --nproc_per_node=$NUM_TRAINERS  YOUR_TRAINING_SCRIPT.py (--arg1 ... train script args...) Fault tolerant (fixed sized number of workers, no elasticity, tolerates 3 failures):   torchrun  --nnodes=$NUM_NODES  --nproc_per_node=$NUM_TRAINERS  --max_restarts=3  --rdzv_id=$JOB_ID  --rdzv_backend=c10d  --rdzv_endpoint=$HOST_NODE_ADDR  YOUR_TRAINING_SCRIPT.py (--arg1 ... train script args...)     torch.distributed.launch的使用也很简单:\n  python -m torch.distributed.launch --nnodes=NNODE --node_rank=NODE_RANK --nproc_per_node=NPROC_PER_NODE \\ YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and all other arguments of your training script)   然后记得你的代码中一定需要设一个--local_rank的参数, torch.distributed.launch会传给你对应的local_rank.\n   import argparse  parser = argparse.ArgumentParser()  parser.add_argument(\"--local_rank\", type=int)  args = parser.parse_args()       if __name__ == \"__main__\":  parser = argparse.ArgumentParser()  parser.add_argument('--seed', type=int)  parser.add_argument('--nproc_per_node', type=int)  parser.add_argument('--nnode', type=int)  parser.add_argument('--node_rank', type=int)  args = parser.parse_args()   mp.spawn(run_demo, args=(args,), nprocs=args.nproc_per_node)   执行时\n Node0: python DDP_test.py --seed 1 --nproc_per_node 1 --nnode 2 --node_rank 0 Node1: python DDP_test.py --seed 1 --nproc_per_node 1 --nnode 2 --node_rank 1    5. 其他注意事项 并行化中的数据集 上文中我们也提到, 我们希望将$N\\times M$个数据不重合地拆分成$N$份, 每份$M$条数据, 为此我们需要使用到DistributedSampler:\n# 载入数据集 train_dataset = torchvision.datasets.MNIST(  root='./data',  train=True,  transform=transforms.ToTensor(),  download=True ) # 配置sampler, 相当于我们希望将原本数据集划分成`world_size`份 # torch.utils.data.distributed.DistributedSampler(dataset, num_replicas=None, rank=None, shuffle=True, seed=0, drop_last=False) train_sampler = torch.utils.data.distributed.DistributedSampler(  train_dataset,  num_replicas=world_size,  rank=global_rank,  seed=seed,  shuffle=True,  pin_memory=True )   num_workers设置了加载数据集的使用的线程数, 0表示只有主进程读取数据集. 关于num_workers的配置可以见知乎:DataLoader的num_workers设置|加速. pin_memory 锁页内存, 可以加速数据读取. (也可能会导致Bug)  并行化中Save和Load模型 (Pytorch) 当使用DDP涉及到保存和读取模型的时候, 我们自然希望的是: 只需要有一个Process保存模型, 同时能够将模型读取到所有Process中.\n在实现中我们需要注意的是:\n 确保Saving的过程结束了才会有Process执行Loading: 借助barrier()实现. 当读取模型的时候, 需要设置正确的map_localtion, 以防止将模型加载到其他Process所处的Device(GPU)上. 因为当没有指定map_location时, Pytorch是默认先把模型参数加载到CPU中, 然后把每个参数复制到它被保存时所在的Device上.  def run_demo_checkpoint(local_rank, args):  # 计算global_rank和world_size  global_rank = local_rank + args.node_rank * args.nproc_per_node  world_size = args.nnode * args.nproc_per_node  setup(global_rank=global_rank, world_size=world_size)  print(f\"Running DDP checkpoint example on rank {global_rank}.\")   # 设置seed  torch.manual_seed(args.seed)   model = ToyModel().to(local_rank)  ddp_model = DDP(model, device_ids=[local_rank])   loss_fn = nn.MSELoss()  optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)   CHECKPOINT_PATH = tempfile.gettempdir() + \"/model.checkpoint\"   if global_rank == 0:  # 只在Process0中保存模型  torch.save(ddp_model.state_dict(), CHECKPOINT_PATH)   # barrier(): 可以理解为只有当所有Process都到达了这一步才能继续往下执行  # 以此保证其他Process只有在Process0完成保存后才可能读取模型  dist.barrier()   # 配置`map_location`.  map_location = torch.device(f'cuda:{local_rank}')   ddp_model.load_state_dict(  torch.load(CHECKPOINT_PATH, map_location=map_location))   optimizer.zero_grad()  outputs = ddp_model(torch.randn(20, 10))  labels = torch.randn(20, 5).to(local_rank)  loss_fn = nn.MSELoss()  loss_fn(outputs, labels).backward()  optimizer.step()  print(outputs)   # Not necessary to use a dist.barrier() to guard the file deletion below  # as the AllReduce ops in the backward pass of DDP already served as  # a synchronization.   if global_rank == 0:  os.remove(CHECKPOINT_PATH)   cleanup() Join: 处理uneven data 我们知道DDP中,Pytorch会在每次backward pass的时候做一次synchronization, 以保证梯度的同步, 但是这就存在一个问题, 如果不同的Process所分配到的数据长度不一样怎么办. 例如Process1中如果有$5$个batch, Process2中只有$6$个batch, 那么Process2在处理最后一个batch的时候就会无限挂起等待其他Process, DDP中提供了Join接口来解决这一问题. (Github Issue)\nimport torch import torch.distributed as dist import os import torch.multiprocessing as mp import torch.nn as nn  def worker(rank):  dist.init_process_group(\"nccl\", rank=rank, world_size=2)  torch.cuda.set_device(rank)  model = nn.Linear(1, 1, bias=False).to(rank)  model = torch.nn.parallel.DistributedDataParallel(  model, device_ids=[rank], output_device=rank  )  # Rank1 比 rank0 会多一个Batch.  inputs = [torch.tensor([1]).float() for _ in range(10 + rank)]  with model.join():  for _ in range(5):  for inp in inputs:  loss = model(inp).sum()  loss.backward()  # 如果没有join() API, 下面的同步语句就会无限挂起等待Rank1的  # allreduce完成.  torch.cuda.synchronize(device=rank)   Join的大致逻辑是, 当某一个Process提前用完了它的数据, 它就会进入joining模式, 从而\"欺骗\"和其他进程之间的allreduce: 比如DDP中用尽数据的Process在梯度allreduce的时候会提供一个全为$0$的梯度用于同步.\n  DDP的join接口具体为: join(divide_by_initial_world_size=True, enable=True, throw_on_early_termination=False)\n  其中divide_by_initial_world_size指的是在average梯度的时候, 除以的是world_size(初始化时候的进程数), 还是现在剩有数据的进程数(即non-joining的进程数). 官方建议是如果不同进程间输入的差异是微小的, 则设置为True; 如果差异非常巨大则设置为False.\n  enable指的是是否能够检测uneven input.\n  throw_on_early_termination指是否在有进程耗尽数据时抛出异常. 如果设置为True, 则会在第一个进程耗尽数据后抛出异常. 注意如果设置了throw_on_early_termination, divide_by_initial_world_size会被忽视.\n Note: If the model or training loop this context manager is wrapped around has additional distributed collective operations, such as SyncBatchNorm in the model’s forward pass, then the flag throw_on_early_termination must be enabled. This is because this context manager is not aware of non-DDP collective communication. This flag will cause all ranks to throw when any one rank exhausts inputs, allowing these errors to be caught and recovered from across all ranks.      Unused Parameters RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter dete ction by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by making sure all `forward` function outputs participate in calculating loss.  If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable). 常用函数   一些可能会使用到的功能函数. (例如在多机上进行验证)\n  all_reduce; all_gather; dist.barrier(), torch.cuda.synchronize(device=local_rank).\n  可以参考:\n PyTorch分布式训练基础–DDP使用 分布式验证 Barrier caused a hang during Evaluation with DDP.: use model.module.    6. 参考资料   PyTorch分布式训练基础–DDP使用\n  知乎:和nn.DataParallel说再见\n  Use nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel\n  Distributed Data Parallel\n  Distributed data parallel training in Pytorch\n  Join部分:\n [源码解析] PyTorch 分布式(11) —– DistributedDataParallel 之 构建Reducer和Join操作 DISTRIBUTED TRAINING WITH UNEVEN INPUTS USING THE JOIN CONTEXT MANAGER      线程和进程的区别: 建议百度. ↩︎\n 当不同卡上处理的数据量($M$)不同时, 不能直接算平均$loss$: Github Issue. ↩︎\n 由于在backward-pass计算梯度时, 该层的梯度不依赖于前面的层, 所以torch.nn.parallel.DistributedDataParallel中各卡上模型参数的同步是跟随着梯度backward-pass同时完成的. ↩︎\n 一个注释, 我在使用nccl的时有时候会出现Socket Timeout的错误, 目前还是一个Open Issue, 有建议说如果不行可以改使用gloo. (Github Issue). ↩︎\n   ","wordCount":"1131","inLanguage":"en","datePublished":"2022-02-14T21:32:33+08:00","dateModified":"2022-02-14T21:32:33+08:00","author":{"@type":"Person","name":"Xinning Zhou"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://coderlemon17.github.io/posts/2022/02-14-ddp/"},"publisher":{"@type":"Organization","name":"Lemon's Blog","logo":{"@type":"ImageObject","url":"https://coderlemon17.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://coderlemon17.github.io/ accesskey=h title="Lemon's Blog (Alt + H)">Lemon's Blog</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://coderlemon17.github.io/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://coderlemon17.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://ml.cs.tsinghua.edu.cn/ title=TSAIL><span>TSAIL</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://coderlemon17.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://coderlemon17.github.io/posts/>Posts</a></div><h1 class=post-title>torch.nn.parallel.DistributedDataParallel: 快速上手</h1><div class=post-meta><span title="2022-02-14 21:32:33 +0800 +0800">February 14, 2022</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;Xinning Zhou&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/posts/2022/02-14-DDP/index.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#0-%e5%89%8d%e8%a8%80 aria-label="0. 前言">0. 前言</a></li><li><a href=#1-%e4%bb%80%e4%b9%88%e6%98%af%e6%95%b0%e6%8d%ae%e5%b9%b6%e8%a1%8c%e5%8c%96 aria-label="1. 什么是数据并行化">1. 什么是数据并行化</a></li><li><a href=#2-%e4%b8%ba%e4%bb%80%e4%b9%88%e8%a6%81%e4%bd%bf%e7%94%a8torchnnparalleldistributeddataparallel aria-label="2. 为什么要使用torch.nn.parallel.DistributedDataParallel">2. 为什么要使用<code>torch.nn.parallel.DistributedDataParallel</code></a></li><li><a href=#3-torchnnparalleldistributeddataparallel-%e7%a9%b6%e7%ab%9f%e5%9c%a8%e5%81%9a%e4%bb%80%e4%b9%88 aria-label="3. torch.nn.parallel.DistributedDataParallel 究竟在做什么?">3. <code>torch.nn.parallel.DistributedDataParallel</code> 究竟在做什么?</a></li><li><a href=#4-torchnnparalleldistributeddataparallel-%e4%bd%bf%e7%94%a8%e8%8c%83%e4%be%8b aria-label="4. torch.nn.parallel.DistributedDataParallel 使用范例">4. <code>torch.nn.parallel.DistributedDataParallel</code> 使用范例</a><ul><ul><li><a href=#%e5%af%bc%e5%85%a5%e9%9c%80%e8%a6%81%e7%9a%84%e5%ba%93 aria-label=导入需要的库>导入需要的库</a></li><li><a href=#%e5%ae%9a%e4%b9%89%e4%b8%80%e4%b8%aa%e7%ae%80%e5%8d%95%e7%9a%84%e6%a8%a1%e5%9e%8b aria-label=定义一个简单的模型>定义一个简单的模型</a></li><li><a href=#%e5%ae%9a%e4%b9%89%e8%ae%ad%e7%bb%83%e8%bf%87%e7%a8%8b aria-label=定义训练过程>定义训练过程</a></li><li><a href=#%e5%a4%9a%e7%ba%bf%e7%a8%8b%e6%89%a7%e8%a1%8c aria-label=多线程执行>多线程执行</a></li></ul></ul></li><li><a href=#5-%e5%85%b6%e4%bb%96%e6%b3%a8%e6%84%8f%e4%ba%8b%e9%a1%b9 aria-label="5. 其他注意事项">5. 其他注意事项</a><ul><ul><li><a href=#%e5%b9%b6%e8%a1%8c%e5%8c%96%e4%b8%ad%e7%9a%84%e6%95%b0%e6%8d%ae%e9%9b%86 aria-label=并行化中的数据集>并行化中的数据集</a></li><li><a href=#%e5%b9%b6%e8%a1%8c%e5%8c%96%e4%b8%adsave%e5%92%8cload%e6%a8%a1%e5%9e%8b-pytorchhttpspytorchorgtutorialsintermediateddp_tutorialhtmlsave-and-load-checkpoints aria-label="并行化中Save和Load模型 (Pytorch)">并行化中Save和Load模型 (<a href=https://pytorch.org/tutorials/intermediate/ddp_tutorial.html#save-and-load-checkpoints>Pytorch</a>)</a></li><li><a href=#join-%e5%a4%84%e7%90%86uneven-data aria-label="Join: 处理uneven data"><code>Join</code>: 处理uneven data</a></li><li><a href=#unused-parameters aria-label="Unused Parameters">Unused Parameters</a></li><li><a href=#%e5%b8%b8%e7%94%a8%e5%87%bd%e6%95%b0 aria-label=常用函数>常用函数</a></li></ul></ul></li><li><a href=#6-%e5%8f%82%e8%80%83%e8%b5%84%e6%96%99 aria-label="6. 参考资料">6. 参考资料</a></li></ul></div></details></div><div class=post-content><h1 id=0-前言>0. 前言<a hidden class=anchor aria-hidden=true href=#0-前言>#</a></h1><p><code>Node</code>: 一个节点, 可以理解为一台电脑.</p><p><code>Device</code>: 工作设备, 可以简单理解为一张卡, 即一个GPU.</p><p><code>Process</code>: 一个进程, 可以简单理解为一个Python程序.</p><p><code>Threading</code>: 一个线程, 一个进程可以有多个线程, 它们共享资源.</p><hr><ol><li>建议: 使用<code>torchrun</code>, 不要使用<code>multiprocessing</code>, <code>torchrun</code>会在程序中断后帮你杀死线程, 使用<code>multiprocessing</code>很容易造成僵尸线程, 资源无法释放.</li></ol><h1 id=1-什么是数据并行化>1. 什么是数据并行化<a hidden class=anchor aria-hidden=true href=#1-什么是数据并行化>#</a></h1><p>随着模型参数和数据量越来越大, 分布式训练成为了深度学习模型训练中越来越重要的一环. 分布式训练包括两类: <strong>模型并行化</strong> 和 <strong>数据并行化.</strong> 在模型并行化中, 一个<code>Device</code>负责处理模型的一个切片 (例如模型的一层); 而在数据并行化中, 一个<code>Device</code>负责处理数据的一个切片 (即Batch的一部分). 我们今天讨论的<code>torch.nn.parallel.DistributedDataParallel</code>就是由pytorch提供的一种数据并行化方式.</p><h1 id=2-为什么要使用torchnnparalleldistributeddataparallel>2. 为什么要使用<code>torch.nn.parallel.DistributedDataParallel</code><a hidden class=anchor aria-hidden=true href=#2-为什么要使用torchnnparalleldistributeddataparallel>#</a></h1><p>相较于<code>torch.nn.parallel.DistributedDataParallel</code>, 一个更易于使用也更被人熟知的接口是<code>torch.nn.DataParallel</code>. 该接口只需要一行修改即可实现"数据并行化" (具体参考<a href=https://zhuanlan.zhihu.com/p/102697821>知乎</a>):</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>device_ids <span style=color:#f92672>=</span> [<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>DataParallel(model, device_ids<span style=color:#f92672>=</span>device_ids)
</span></span></code></pre></div><p>此方法虽然简单, 但是存在若干问题, 例如设备间负载不均; 效率不高等. 现在官方推荐的方法为<code>torch.nn.parallel.DistributedDataParallel</code> ( <a href=https://pytorch.org/docs/stable/notes/cuda.html#cuda-nn-ddp-instead>Use nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel</a> ).</p><p>为何如此呢? 简单来说, <code>torch.nn.DataParallel</code> 实现的是单机-多线程 (<em>Single-Node Multi-threading</em>), 而<code>torch.nn.parallel.DistributedDataParallel</code> 实现的是 单机/多机-多进程 (Single/Multi-Node Multi-process). 即<code>torch.nn.parallel.DistributedDataParallel</code>中的每一个模型是由一个独立的<code>Process</code>来控制的.<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></p><p>此外, 官方文档中指出<code>torch.nn.parallel.DistributedDataParallel</code>是和模型并行化兼容的, 而<code>torch.nn.DataParallel</code>则不可以.</p><h1 id=3-torchnnparalleldistributeddataparallel-究竟在做什么>3. <code>torch.nn.parallel.DistributedDataParallel</code> 究竟在做什么?<a hidden class=anchor aria-hidden=true href=#3-torchnnparalleldistributeddataparallel-究竟在做什么>#</a></h1><figure class=align-center><img loading=lazy src=assets/image-20220206160547-4nzpgxt.png#center width=30%><figcaption style=text-align:center;color:gray;font-weight:400>Fig 1. torch.nn.parallel.DistributedDataParallel (https://yangkky.github.io/2019/07/08/distributed-pytorch-tutorial.html)</figcaption></figure><p>简单来说, 如果我们有$N$个<code>Device</code> (即$N$张卡), 每次的batch有$N\times M$个数据, 那我们如果能将模型分别复制到这$N$张卡上, 每张卡负责计算$M$个数据的$loss$的平均梯度, 然后将这些$N$张卡上梯度平均起来<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>, 同时<strong>将梯度</strong>更新到所有模型上<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>. 那我们相当于花了计算$M$个数据的时间, 完成了$N\times M$条数据的计算.</p><p>而多机希望实现的是: 简单来说, 就是你只需要知道我有多少个<code>Device</code>, 而不需要管这些<code>Device</code>分布在多少个<code>Node</code>上.</p><p>而为了保证模型间参数的同步, 多设备间需要通讯, 这是通过后端 (<code>Backend</code> 来完成的, 见<a href=https://pytorch.org/docs/stable/distributed.html>torch.distributed</a>), 简单来说: 如果想使用GPU训练用<code>nccl</code>, 如果使用CPU用<code>gloo</code><sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>.</p><ul><li>Each process maintains its own optimizer and performs a complete optimization step with each iteration. While this may appear redundant, since the gradients have already been gathered together and averaged across processes and are thus the same for every process, this means that no parameter broadcast step is needed, reducing time spent transferring tensors between nodes.</li><li>Each process contains an independent Python interpreter, eliminating the extra interpreter overhead and “GIL-thrashing” that comes from driving several execution threads, model replicas, or GPUs from a single Python process. This is especially important for models that make heavy use of the Python runtime, including models with recurrent layers or many small components.</li></ul><h1 id=4-torchnnparalleldistributeddataparallel-使用范例>4. <code>torch.nn.parallel.DistributedDataParallel</code> 使用范例<a hidden class=anchor aria-hidden=true href=#4-torchnnparalleldistributeddataparallel-使用范例>#</a></h1><h3 id=导入需要的库>导入需要的库<a hidden class=anchor aria-hidden=true href=#导入需要的库>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> argparse
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.distributed <span style=color:#66d9ef>as</span> dist
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.multiprocessing <span style=color:#66d9ef>as</span> mp
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> torch.nn.parallel <span style=color:#f92672>import</span> DistributedDataParallel <span style=color:#66d9ef>as</span> DDP
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.optim <span style=color:#66d9ef>as</span> optim
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.distributed <span style=color:#66d9ef>as</span> dist
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> datetime <span style=color:#f92672>import</span> timedelta
</span></span></code></pre></div><h3 id=定义一个简单的模型>定义一个简单的模型<a hidden class=anchor aria-hidden=true href=#定义一个简单的模型>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>ToyModel</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self):
</span></span><span style=display:flex><span>        super(ToyModel, self)<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>net1 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>10</span>)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>relu <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>ReLU()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>net2 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>5</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>net2(self<span style=color:#f92672>.</span>relu(self<span style=color:#f92672>.</span>net1(x)))
</span></span></code></pre></div><p>Fig 1. 中的多个<code>Process</code>形成了一个<code>Process group</code>, 在使用<code>torch.nn.parallel.DistributedDataParallel</code>之间我们需要初始化它, 初始化需要两个参数<code>global_rank</code> 和 <code>world_size</code>:</p><ul><li><p>其中<code>world_size</code>是指你一共有多少<code>Process</code>, 即<code>world_size = 节点数量 * 每个节点上有多少Process = nnode * nproc_per_node</code>.</p></li><li><p>而对于每一个<code>Process</code>, 它都有一个<code>local_rank</code>和<code>global_rank</code>, <code>local_rank</code>对应的就是该<code>Process</code>在自己的<code>Node</code>上的编号, 而<code>global_rank</code>就是全局的编号.</p><ul><li>比如你有$2$个<code>Node</code>, 每个<code>Node</code>上各有$2$个<code>Proess</code> (<em>Process0, Process1, Process2, Process3</em>). 那么对于<em>Process2</em>来说, 它的<code>local_rank</code>就是$0$ (即它在<code>Node1</code>上是第$0$个<code>Process</code>), <code>global_rank</code> 就是$2$.</li><li>不难发现, <code>local_rank</code>对应的就是该<code>Process</code>需要使用的<code>Device</code>(GPU)编号 (并不一定, 但这是一种方便的方法).</li></ul></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>setup</span>(global_rank, world_size):
</span></span><span style=display:flex><span>    <span style=color:#75715e># 配置Master Node的信息</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># os.environ[&#39;MASTER_ADDR&#39;] = &#39;localhost&#39;</span>
</span></span><span style=display:flex><span>    os<span style=color:#f92672>.</span>environ[<span style=color:#e6db74>&#39;MASTER_ADDR&#39;</span>] <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;XXX.XXX.XXX.XXX&#39;</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># os.environ[&#39;MASTER_PORT&#39;] = &#39;23555&#39;</span>
</span></span><span style=display:flex><span>    os<span style=color:#f92672>.</span>environ[<span style=color:#e6db74>&#39;MASTER_PORT&#39;</span>] <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;XXXX&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 初始化Process Group</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 关于init_method, 参数详见https://pytorch.org/docs/stable/distributed.html#initialization</span>
</span></span><span style=display:flex><span>    dist<span style=color:#f92672>.</span>init_process_group(<span style=color:#e6db74>&#34;nccl&#34;</span>, init_method<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;env://&#39;</span>, rank<span style=color:#f92672>=</span>global_rank, world_size<span style=color:#f92672>=</span>world_size, timeout<span style=color:#f92672>=</span>timedelta(seconds<span style=color:#f92672>=</span><span style=color:#ae81ff>5</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>cleanup</span>():
</span></span><span style=display:flex><span>    dist<span style=color:#f92672>.</span>destroy_process_group()
</span></span></code></pre></div><ul><li>对于<code>Process group</code>来说需要有一个<code>Master node</code>, 可以理解为是这个<code>Process group</code>的根节点, 我们一般配置为为<code>Node0</code>, 在后续在各个节点上启动代码时, 最好也是先从<code>Node0</code>开始启动. 需要确保<code>Master node</code>的IP地址可访问, 且端口没有被占用.</li><li>对于响应超时可以自行设置超时时间<code>timeout</code>, 官方建议设置一个较大的时间, 以防可能出现的网络延迟.</li></ul><h3 id=定义训练过程>定义训练过程<a hidden class=anchor aria-hidden=true href=#定义训练过程>#</a></h3><p>接下来我们定义在每一个<code>Process</code>中我们希望执行的代码.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>run_demo</span>(local_rank, args):
</span></span><span style=display:flex><span>    <span style=color:#75715e># 计算global_rank和world_size</span>
</span></span><span style=display:flex><span>    global_rank <span style=color:#f92672>=</span> local_rank <span style=color:#f92672>+</span> args<span style=color:#f92672>.</span>node_rank <span style=color:#f92672>*</span> args<span style=color:#f92672>.</span>nproc_per_node
</span></span><span style=display:flex><span>    world_size <span style=color:#f92672>=</span> args<span style=color:#f92672>.</span>nnode <span style=color:#f92672>*</span> args<span style=color:#f92672>.</span>nproc_per_node
</span></span><span style=display:flex><span>    setup(global_rank<span style=color:#f92672>=</span>global_rank, world_size<span style=color:#f92672>=</span>world_size)
</span></span><span style=display:flex><span>    <span style=color:#75715e># 设置seed</span>
</span></span><span style=display:flex><span>    torch<span style=color:#f92672>.</span>manual_seed(args<span style=color:#f92672>.</span>seed)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 创建模型, 并将其移动到local_rank对应的GPU上</span>
</span></span><span style=display:flex><span>    model <span style=color:#f92672>=</span> ToyModel()<span style=color:#f92672>.</span>to(local_rank)
</span></span><span style=display:flex><span>    ddp_model <span style=color:#f92672>=</span> DDP(model, device_ids<span style=color:#f92672>=</span>[local_rank], output_device<span style=color:#f92672>=</span>local_rank)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    loss_fn <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>MSELoss()
</span></span><span style=display:flex><span>    optimizer <span style=color:#f92672>=</span> optim<span style=color:#f92672>.</span>SGD(ddp_model<span style=color:#f92672>.</span>parameters(), lr<span style=color:#f92672>=</span><span style=color:#ae81ff>0.001</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    optimizer<span style=color:#f92672>.</span>zero_grad()
</span></span><span style=display:flex><span>    outputs <span style=color:#f92672>=</span> ddp_model(torch<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>20</span>, <span style=color:#ae81ff>10</span>))
</span></span><span style=display:flex><span>    labels <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>20</span>, <span style=color:#ae81ff>5</span>)<span style=color:#f92672>.</span>to(local_rank)
</span></span><span style=display:flex><span>    loss_fn(outputs, labels)<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>    optimizer<span style=color:#f92672>.</span>step()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    print([data <span style=color:#66d9ef>for</span> data <span style=color:#f92672>in</span> model<span style=color:#f92672>.</span>parameters()])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    cleanup()
</span></span></code></pre></div><ul><li><p>需要注意, 我们每一个<code>Process</code>的应该运行在它自己对应的GPU上, 所以我们在代码中需要加上以下三种方式的一种:</p><ul><li><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># 方法1</span>
</span></span><span style=display:flex><span>torch<span style=color:#f92672>.</span>cuda<span style=color:#f92672>.</span>set_device(local_rank)
</span></span><span style=display:flex><span><span style=color:#75715e># 方法2</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>with</span> torch<span style=color:#f92672>.</span>cuda<span style=color:#f92672>.</span>device(local_rank)
</span></span><span style=display:flex><span><span style=color:#75715e># 方法3</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> ToyModel()<span style=color:#f92672>.</span>to(local_rank)
</span></span></code></pre></div></li></ul></li><li><p>在<code>DDP</code>中, 所有模型都是以相同的参数被初始化, 同时训练过程中的<strong>梯度</strong>会在backward pass中被同步, 这就保证了在<code>optimizer</code>的优化过程中所有模型的参数保持一致.</p></li></ul><h3 id=多线程执行>多线程执行<a hidden class=anchor aria-hidden=true href=#多线程执行>#</a></h3><p>最后, 我们需要在每一个<code>Node</code>上启动<code>nproc_per_node</code>个<code>Process</code>, 这一步可以使用<a href=https://pytorch.org/docs/stable/distributed.html#launch-utility>torch.distributed.launch</a>/<a href=https://pytorch.org/docs/stable/elastic/run.html#launcher-api>torchrun</a>/multiprocessing来实现:</p><ul><li><p>虽然为了代码清晰这里我们用了<code>multiprocessing</code>来实现, 但是在代码意外退出的时候它容易出现僵尸进程等Bug (<a href=https://github.com/pytorch/pytorch/issues/48382>Github</a>), 在工程代码中不建议使用.</p></li><li><p><code>torchrun</code>是为了替代<code>torch.distributed.launch</code>的新型启动方式, 可以支持<a href=https://pytorch.org/docs/stable/elastic/run.html#>ELASTIC LAUNCH</a>, 即动态控制启动的节点数量, 但是由于是新功能, 只有最新的<code>torch 1.10</code>, 处于兼容性考虑还是建议使用<code>torch.distributed.launch</code>.</p><ul><li><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>Single-node multi-worker
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>&gt;&gt;&gt; torchrun
</span></span><span style=display:flex><span>    --standalone
</span></span><span style=display:flex><span>    --nnodes<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>    --nproc_per_node<span style=color:#f92672>=</span>$NUM_TRAINERS
</span></span><span style=display:flex><span>    YOUR_TRAINING_SCRIPT.py <span style=color:#f92672>(</span>--arg1 ... train script args...<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>Fault tolerant <span style=color:#f92672>(</span>fixed sized number of workers, no elasticity, tolerates <span style=color:#ae81ff>3</span> failures<span style=color:#f92672>)</span>:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>&gt;&gt;&gt; torchrun
</span></span><span style=display:flex><span>    --nnodes<span style=color:#f92672>=</span>$NUM_NODES
</span></span><span style=display:flex><span>    --nproc_per_node<span style=color:#f92672>=</span>$NUM_TRAINERS
</span></span><span style=display:flex><span>    --max_restarts<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>
</span></span><span style=display:flex><span>    --rdzv_id<span style=color:#f92672>=</span>$JOB_ID
</span></span><span style=display:flex><span>    --rdzv_backend<span style=color:#f92672>=</span>c10d
</span></span><span style=display:flex><span>    --rdzv_endpoint<span style=color:#f92672>=</span>$HOST_NODE_ADDR
</span></span><span style=display:flex><span>    YOUR_TRAINING_SCRIPT.py <span style=color:#f92672>(</span>--arg1 ... train script args...<span style=color:#f92672>)</span>
</span></span></code></pre></div></li></ul></li><li><p><code>torch.distributed.launch</code>的使用也很简单:</p><ul><li><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>python -m torch.distributed.launch --nnodes<span style=color:#f92672>=</span>NNODE --node_rank<span style=color:#f92672>=</span>NODE_RANK --nproc_per_node<span style=color:#f92672>=</span>NPROC_PER_NODE <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>YOUR_TRAINING_SCRIPT.py <span style=color:#f92672>(</span>--arg1 --arg2 --arg3 and all other arguments of your training script<span style=color:#f92672>)</span>
</span></span></code></pre></div></li><li><p>然后记得你的代码中<strong>一定需要</strong>设一个<code>--local_rank</code>的参数, <code>torch.distributed.launch</code>会传给你对应的<code>local_rank</code>.</p><ul><li><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> <span style=color:#f92672>import</span> argparse
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> parser <span style=color:#f92672>=</span> argparse<span style=color:#f92672>.</span>ArgumentParser()
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> parser<span style=color:#f92672>.</span>add_argument(<span style=color:#e6db74>&#34;--local_rank&#34;</span>, type<span style=color:#f92672>=</span>int)
</span></span><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> args <span style=color:#f92672>=</span> parser<span style=color:#f92672>.</span>parse_args()
</span></span></code></pre></div></li></ul></li></ul></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    parser <span style=color:#f92672>=</span> argparse<span style=color:#f92672>.</span>ArgumentParser()
</span></span><span style=display:flex><span>    parser<span style=color:#f92672>.</span>add_argument(<span style=color:#e6db74>&#39;--seed&#39;</span>, type<span style=color:#f92672>=</span>int)
</span></span><span style=display:flex><span>    parser<span style=color:#f92672>.</span>add_argument(<span style=color:#e6db74>&#39;--nproc_per_node&#39;</span>, type<span style=color:#f92672>=</span>int)
</span></span><span style=display:flex><span>    parser<span style=color:#f92672>.</span>add_argument(<span style=color:#e6db74>&#39;--nnode&#39;</span>, type<span style=color:#f92672>=</span>int)
</span></span><span style=display:flex><span>    parser<span style=color:#f92672>.</span>add_argument(<span style=color:#e6db74>&#39;--node_rank&#39;</span>, type<span style=color:#f92672>=</span>int)
</span></span><span style=display:flex><span>    args <span style=color:#f92672>=</span> parser<span style=color:#f92672>.</span>parse_args()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    mp<span style=color:#f92672>.</span>spawn(run_demo, args<span style=color:#f92672>=</span>(args,), nprocs<span style=color:#f92672>=</span>args<span style=color:#f92672>.</span>nproc_per_node)
</span></span></code></pre></div><ul><li><p>执行时</p><ul><li><code>Node0</code>: <code>python DDP_test.py --seed 1 --nproc_per_node 1 --nnode 2 --node_rank 0</code></li><li><code>Node1</code>: <code>python DDP_test.py --seed 1 --nproc_per_node 1 --nnode 2 --node_rank 1</code></li></ul></li></ul><h1 id=5-其他注意事项>5. 其他注意事项<a hidden class=anchor aria-hidden=true href=#5-其他注意事项>#</a></h1><h3 id=并行化中的数据集>并行化中的数据集<a hidden class=anchor aria-hidden=true href=#并行化中的数据集>#</a></h3><p>上文中我们也提到, 我们希望将$N\times M$个数据<strong>不重合地</strong>拆分成$N$份, 每份$M$条数据, 为此我们需要使用到<code>DistributedSampler</code>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># 载入数据集</span>
</span></span><span style=display:flex><span>train_dataset <span style=color:#f92672>=</span> torchvision<span style=color:#f92672>.</span>datasets<span style=color:#f92672>.</span>MNIST(
</span></span><span style=display:flex><span>    root<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;./data&#39;</span>,
</span></span><span style=display:flex><span>    train<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>    transform<span style=color:#f92672>=</span>transforms<span style=color:#f92672>.</span>ToTensor(),
</span></span><span style=display:flex><span>    download<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span><span style=color:#75715e># 配置sampler, 相当于我们希望将原本数据集划分成`world_size`份</span>
</span></span><span style=display:flex><span><span style=color:#75715e># torch.utils.data.distributed.DistributedSampler(dataset, num_replicas=None, rank=None, shuffle=True, seed=0, drop_last=False)</span>
</span></span><span style=display:flex><span>train_sampler <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>utils<span style=color:#f92672>.</span>data<span style=color:#f92672>.</span>distributed<span style=color:#f92672>.</span>DistributedSampler(
</span></span><span style=display:flex><span>    train_dataset,
</span></span><span style=display:flex><span>    num_replicas<span style=color:#f92672>=</span>world_size,
</span></span><span style=display:flex><span>    rank<span style=color:#f92672>=</span>global_rank,
</span></span><span style=display:flex><span>    seed<span style=color:#f92672>=</span>seed,
</span></span><span style=display:flex><span>    shuffle<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>    pin_memory<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p><img src=assets/image-20220214135043-51buunq.png alt=image.png></p><ul><li><code>num_workers</code>设置了加载数据集的使用的线程数, <code>0</code>表示只有主进程读取数据集. 关于<code>num_workers</code>的配置可以见<a href=https://zhuanlan.zhihu.com/p/366595260>知乎:DataLoader的num_workers设置|加速</a>.</li><li><code>pin_memory</code> 锁页内存, 可以加速数据读取. (也可能会导致Bug)</li></ul><h3 id=并行化中save和load模型-pytorchhttpspytorchorgtutorialsintermediateddp_tutorialhtmlsave-and-load-checkpoints>并行化中Save和Load模型 (<a href=https://pytorch.org/tutorials/intermediate/ddp_tutorial.html#save-and-load-checkpoints>Pytorch</a>)<a hidden class=anchor aria-hidden=true href=#并行化中save和load模型-pytorchhttpspytorchorgtutorialsintermediateddp_tutorialhtmlsave-and-load-checkpoints>#</a></h3><p>当使用<code>DDP</code>涉及到保存和读取模型的时候, 我们自然希望的是: <strong>只需要有一个<code>Process</code>保存模型</strong>, <strong>同时能够将模型读取到所有<code>Process</code>中</strong>.</p><p>在实现中我们需要注意的是:</p><ul><li>确保<code>Saving</code>的过程结束了才会有<code>Process</code>执行<code>Loading</code>: 借助<code>barrier()</code>实现.</li><li>当读取模型的时候, 需要设置正确的<code>map_localtion</code>, 以防止将模型加载到其他<code>Process</code>所处的<code>Device</code>(GPU)上. 因为当没有指定<code>map_location</code>时, Pytorch是默认先把模型参数加载到CPU中, 然后把每个参数复制到它被保存时所在的<code>Device</code>上.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>run_demo_checkpoint</span>(local_rank, args):
</span></span><span style=display:flex><span>    <span style=color:#75715e># 计算global_rank和world_size</span>
</span></span><span style=display:flex><span>    global_rank <span style=color:#f92672>=</span> local_rank <span style=color:#f92672>+</span> args<span style=color:#f92672>.</span>node_rank <span style=color:#f92672>*</span> args<span style=color:#f92672>.</span>nproc_per_node
</span></span><span style=display:flex><span>    world_size <span style=color:#f92672>=</span> args<span style=color:#f92672>.</span>nnode <span style=color:#f92672>*</span> args<span style=color:#f92672>.</span>nproc_per_node
</span></span><span style=display:flex><span>    setup(global_rank<span style=color:#f92672>=</span>global_rank, world_size<span style=color:#f92672>=</span>world_size)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Running DDP checkpoint example on rank </span><span style=color:#e6db74>{</span>global_rank<span style=color:#e6db74>}</span><span style=color:#e6db74>.&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 设置seed</span>
</span></span><span style=display:flex><span>    torch<span style=color:#f92672>.</span>manual_seed(args<span style=color:#f92672>.</span>seed)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    model <span style=color:#f92672>=</span> ToyModel()<span style=color:#f92672>.</span>to(local_rank)
</span></span><span style=display:flex><span>    ddp_model <span style=color:#f92672>=</span> DDP(model, device_ids<span style=color:#f92672>=</span>[local_rank])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    loss_fn <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>MSELoss()
</span></span><span style=display:flex><span>    optimizer <span style=color:#f92672>=</span> optim<span style=color:#f92672>.</span>SGD(ddp_model<span style=color:#f92672>.</span>parameters(), lr<span style=color:#f92672>=</span><span style=color:#ae81ff>0.001</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    CHECKPOINT_PATH <span style=color:#f92672>=</span> tempfile<span style=color:#f92672>.</span>gettempdir() <span style=color:#f92672>+</span> <span style=color:#e6db74>&#34;/model.checkpoint&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> global_rank <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>        <span style=color:#75715e># 只在Process0中保存模型</span>
</span></span><span style=display:flex><span>        torch<span style=color:#f92672>.</span>save(ddp_model<span style=color:#f92672>.</span>state_dict(), CHECKPOINT_PATH)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># barrier(): 可以理解为只有当所有Process都到达了这一步才能继续往下执行</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 以此保证其他Process只有在Process0完成保存后才可能读取模型</span>
</span></span><span style=display:flex><span>    dist<span style=color:#f92672>.</span>barrier()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 配置`map_location`.</span>
</span></span><span style=display:flex><span>    map_location <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>device(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;cuda:</span><span style=color:#e6db74>{</span>local_rank<span style=color:#e6db74>}</span><span style=color:#e6db74>&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    ddp_model<span style=color:#f92672>.</span>load_state_dict(
</span></span><span style=display:flex><span>        torch<span style=color:#f92672>.</span>load(CHECKPOINT_PATH, map_location<span style=color:#f92672>=</span>map_location))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    optimizer<span style=color:#f92672>.</span>zero_grad()
</span></span><span style=display:flex><span>    outputs <span style=color:#f92672>=</span> ddp_model(torch<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>20</span>, <span style=color:#ae81ff>10</span>))
</span></span><span style=display:flex><span>    labels <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>20</span>, <span style=color:#ae81ff>5</span>)<span style=color:#f92672>.</span>to(local_rank)
</span></span><span style=display:flex><span>    loss_fn <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>MSELoss()
</span></span><span style=display:flex><span>    loss_fn(outputs, labels)<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>    optimizer<span style=color:#f92672>.</span>step()
</span></span><span style=display:flex><span>    print(outputs)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Not necessary to use a dist.barrier() to guard the file deletion below</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># as the AllReduce ops in the backward pass of DDP already served as</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># a synchronization.</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> global_rank <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>        os<span style=color:#f92672>.</span>remove(CHECKPOINT_PATH)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    cleanup()
</span></span></code></pre></div><h3 id=join-处理uneven-data><code>Join</code>: 处理uneven data<a hidden class=anchor aria-hidden=true href=#join-处理uneven-data>#</a></h3><p>我们知道<code>DDP</code>中,Pytorch会在每次backward pass的时候做一次synchronization, 以保证梯度的同步, 但是这就存在一个问题, 如果不同的<code>Process</code>所分配到的数据长度不一样怎么办. 例如<code>Process1</code>中如果有$5$个batch, <code>Process2</code>中只有$6$个batch, 那么<code>Process2</code>在处理最后一个batch的时候就会无限挂起等待其他<code>Process</code>, <code>DDP</code>中提供了<code>Join</code>接口来解决这一问题. (<a href=https://github.com/pytorch/pytorch/issues/38174>Github Issue</a>)</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.distributed <span style=color:#66d9ef>as</span> dist
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.multiprocessing <span style=color:#66d9ef>as</span> mp
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>worker</span>(rank):
</span></span><span style=display:flex><span>    dist<span style=color:#f92672>.</span>init_process_group(<span style=color:#e6db74>&#34;nccl&#34;</span>, rank<span style=color:#f92672>=</span>rank, world_size<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>    torch<span style=color:#f92672>.</span>cuda<span style=color:#f92672>.</span>set_device(rank)
</span></span><span style=display:flex><span>    model <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, bias<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)<span style=color:#f92672>.</span>to(rank)
</span></span><span style=display:flex><span>    model <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>parallel<span style=color:#f92672>.</span>DistributedDataParallel(
</span></span><span style=display:flex><span>        model, device_ids<span style=color:#f92672>=</span>[rank], output_device<span style=color:#f92672>=</span>rank
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    <span style=color:#75715e># Rank1 比 rank0 会多一个Batch.</span>
</span></span><span style=display:flex><span>    inputs <span style=color:#f92672>=</span> [torch<span style=color:#f92672>.</span>tensor([<span style=color:#ae81ff>1</span>])<span style=color:#f92672>.</span>float() <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>10</span> <span style=color:#f92672>+</span> rank)]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> model<span style=color:#f92672>.</span>join():
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>5</span>):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> inp <span style=color:#f92672>in</span> inputs:
</span></span><span style=display:flex><span>                loss <span style=color:#f92672>=</span> model(inp)<span style=color:#f92672>.</span>sum()
</span></span><span style=display:flex><span>                loss<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>    <span style=color:#75715e># 如果没有join() API, 下面的同步语句就会无限挂起等待Rank1的</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># allreduce完成.</span>
</span></span><span style=display:flex><span>    torch<span style=color:#f92672>.</span>cuda<span style=color:#f92672>.</span>synchronize(device<span style=color:#f92672>=</span>rank)
</span></span></code></pre></div><ul><li><p><code>Join</code>的大致逻辑是, 当某一个<code>Process</code>提前用完了它的数据, 它就会进入joining模式, 从而"欺骗"和其他进程之间的allreduce: 比如<code>DDP</code>中用尽数据的<code>Process</code>在梯度allreduce的时候会提供一个全为$0$的梯度用于同步.</p></li><li><p><code>DDP</code>的<code>join</code><a href=https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel.join>接口</a>具体为: <code>join(divide_by_initial_world_size=True, enable=True, throw_on_early_termination=False)</code></p><ul><li><p>其中<code>divide_by_initial_world_size</code>指的是在average梯度的时候, 除以的是<code>world_size</code>(初始化时候的进程数), 还是现在剩有数据的进程数(即non-joining的进程数). <a href=https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel.join>官方建议</a>是如果不同进程间输入的差异是微小的, 则设置为<code>True</code>; 如果差异非常巨大则设置为<code>False</code>.</p></li><li><p><code>enable</code>指的是是否能够检测uneven input.</p></li><li><p><code>throw_on_early_termination</code>指是否在有进程耗尽数据时抛出异常. 如果设置为<code>True</code>, 则会在第一个进程耗尽数据后抛出异常. 注意如果设置了<code>throw_on_early_termination</code>, <code>divide_by_initial_world_size</code>会被忽视.</p><ul><li>Note: If the model or training loop this context manager is wrapped around has additional distributed collective operations, such as <code>SyncBatchNorm</code> in the model&rsquo;s forward pass, then the flag <code>throw_on_early_termination</code> must be enabled. This is because this context manager is not aware of <strong>non-DDP</strong> collective communication. This flag will cause all ranks to throw when any one rank exhausts inputs, allowing these errors to be caught and recovered from across all ranks.</li></ul></li></ul></li></ul><h3 id=unused-parameters>Unused Parameters<a hidden class=anchor aria-hidden=true href=#unused-parameters>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter dete
</span></span><span style=display:flex><span>ction by passing the keyword argument <span style=color:#e6db74>`</span>find_unused_parameters<span style=color:#f92672>=</span>True<span style=color:#e6db74>`</span> to <span style=color:#e6db74>`</span>torch.nn.parallel.DistributedDataParallel<span style=color:#e6db74>`</span>, and by                                                                                                   
</span></span><span style=display:flex><span>making sure all <span style=color:#e6db74>`</span>forward<span style=color:#e6db74>`</span> <span style=color:#66d9ef>function</span> outputs participate in calculating loss.                                                                                                                                                  
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>If you already have <span style=color:#66d9ef>done</span> the above, <span style=color:#66d9ef>then</span> the distributed data parallel module wasn<span style=color:#e6db74>&#39;t able to locate the output tensors in the return value of your module&#39;</span>s <span style=color:#e6db74>`</span>forward<span style=color:#e6db74>`</span> <span style=color:#66d9ef>function</span>. Please include the loss <span style=color:#66d9ef>function</span> and the structure
</span></span><span style=display:flex><span>of the <span style=color:#66d9ef>return</span> value of <span style=color:#e6db74>`</span>forward<span style=color:#e6db74>`</span> of your module when reporting this issue <span style=color:#f92672>(</span>e.g. list, dict, iterable<span style=color:#f92672>)</span>.
</span></span></code></pre></div><h3 id=常用函数>常用函数<a hidden class=anchor aria-hidden=true href=#常用函数>#</a></h3><ul><li><p>一些可能会使用到的功能函数. (例如在多机上进行验证)</p></li><li><p><code>all_reduce</code>; <code>all_gather</code>; <code>dist.barrier()</code>, <code>torch.cuda.synchronize(device=local_rank)</code>.</p></li><li><p>可以参考:</p><ul><li><a href=https://zhuanlan.zhihu.com/p/358974461>PyTorch分布式训练基础&ndash;DDP使用</a></li><li><a href=https://zhuanlan.zhihu.com/p/98535650>分布式验证</a></li><li><a href=https://github.com/pytorch/pytorch/issues/54059>Barrier caused a hang during Evaluation with DDP.</a>: use <code>model.module</code>.</li></ul></li></ul><h1 id=6-参考资料>6. 参考资料<a hidden class=anchor aria-hidden=true href=#6-参考资料>#</a></h1><ul><li><p><a href=https://zhuanlan.zhihu.com/p/358974461>PyTorch分布式训练基础&ndash;DDP使用</a></p></li><li><p><a href=https://zhuanlan.zhihu.com/p/95700549>知乎:和nn.DataParallel说再见</a></p></li><li><p><a href=https://pytorch.org/docs/stable/notes/cuda.html#cuda-nn-ddp-instead>Use nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel</a></p></li><li><p><a href=https://pytorch.org/docs/stable/notes/ddp.html#ddp>Distributed Data Parallel</a></p></li><li><p><a href=https://yangkky.github.io/2019/07/08/distributed-pytorch-tutorial.html>Distributed data parallel training in Pytorch</a></p></li><li><p><code>Join部分</code>:</p><ul><li><a href=https://www.cnblogs.com/rossiXYZ/p/15584560.html>[源码解析] PyTorch 分布式(11) &mdash;&ndash; DistributedDataParallel 之 构建Reducer和Join操作</a></li><li><a href=https://pytorch.org/tutorials/advanced/generic_join.html>DISTRIBUTED TRAINING WITH UNEVEN INPUTS USING THE JOIN CONTEXT MANAGER</a></li></ul></li></ul><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>线程和进程的区别: 建议百度.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>当不同卡上处理的数据量($M$)不同时, 不能直接算平均$loss$: <a href=https://github.com/pytorch/pytorch/issues/9811>Github Issue</a>.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>由于在backward-pass计算梯度时, 该层的梯度不依赖于前面的层, 所以<code>torch.nn.parallel.DistributedDataParallel</code>中各卡上模型参数的同步是跟随着梯度backward-pass同时完成的.&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>一个注释, 我在使用<code>nccl</code>的时有时候会出现<code>Socket Timeout</code>的错误, 目前还是一个Open Issue, 有建议说如果不行可以改使用<code>gloo</code>. (<a href=https://github.com/pytorch/pytorch/issues/25767>Github Issue</a>).&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://coderlemon17.github.io/tags/torch/>torch</a></li></ul><nav class=paginav><a class=prev href=https://coderlemon17.github.io/posts/2022/05-11-mcmc/><span class=title>« Prev</span><br><span>详解Markov Chain Monte Carlo (MCMC): 从拒绝-接受采样到Gibbs Sampling</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share torch.nn.parallel.DistributedDataParallel: 快速上手 on twitter" href="https://twitter.com/intent/tweet/?text=torch.nn.parallel.DistributedDataParallel%3a%20%e5%bf%ab%e9%80%9f%e4%b8%8a%e6%89%8b&url=https%3a%2f%2fcoderlemon17.github.io%2fposts%2f2022%2f02-14-ddp%2f&hashtags=torch"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share torch.nn.parallel.DistributedDataParallel: 快速上手 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fcoderlemon17.github.io%2fposts%2f2022%2f02-14-ddp%2f&title=torch.nn.parallel.DistributedDataParallel%3a%20%e5%bf%ab%e9%80%9f%e4%b8%8a%e6%89%8b&summary=torch.nn.parallel.DistributedDataParallel%3a%20%e5%bf%ab%e9%80%9f%e4%b8%8a%e6%89%8b&source=https%3a%2f%2fcoderlemon17.github.io%2fposts%2f2022%2f02-14-ddp%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share torch.nn.parallel.DistributedDataParallel: 快速上手 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fcoderlemon17.github.io%2fposts%2f2022%2f02-14-ddp%2f&title=torch.nn.parallel.DistributedDataParallel%3a%20%e5%bf%ab%e9%80%9f%e4%b8%8a%e6%89%8b"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share torch.nn.parallel.DistributedDataParallel: 快速上手 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fcoderlemon17.github.io%2fposts%2f2022%2f02-14-ddp%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share torch.nn.parallel.DistributedDataParallel: 快速上手 on whatsapp" href="https://api.whatsapp.com/send?text=torch.nn.parallel.DistributedDataParallel%3a%20%e5%bf%ab%e9%80%9f%e4%b8%8a%e6%89%8b%20-%20https%3a%2f%2fcoderlemon17.github.io%2fposts%2f2022%2f02-14-ddp%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share torch.nn.parallel.DistributedDataParallel: 快速上手 on telegram" href="https://telegram.me/share/url?text=torch.nn.parallel.DistributedDataParallel%3a%20%e5%bf%ab%e9%80%9f%e4%b8%8a%e6%89%8b&url=https%3a%2f%2fcoderlemon17.github.io%2fposts%2f2022%2f02-14-ddp%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://coderlemon17.github.io/>Lemon's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>