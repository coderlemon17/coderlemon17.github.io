<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>详解Markov Chain Monte Carlo (MCMC): 从拒绝-接受采样到Gibbs Sampling | Lemon's Blog</title><meta name=keywords content="math"><meta name=description content="0. Main Takeaway Sec 1. 介绍了蒙特卡洛采样 (MC) 和拒绝-接受采样. Sec 2. 介绍了马尔可夫链(MC)和基于马尔可夫链的采样. Sec 3. 介绍了马尔可夫链蒙特卡洛采样(MCM"><meta name=author content="Xinning Zhou"><link rel=canonical href=https://coderlemon17.github.io/posts/2022/05-11-mcmc/><link crossorigin=anonymous href=/assets/css/stylesheet.min.8dde94775270f9778dab857161bf6801bfb70fb40da921d627a3bf5243c1c47d.css integrity="sha256-jd6Ud1Jw+XeNq4VxYb9oAb+3D7QNqSHWJ6O/UkPBxH0=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.b95bacdc39e37a332a9f883b1e78be4abc1fdca2bc1f2641f55e3cd3dabd4d61.js integrity="sha256-uVus3DnjejMqn4g7Hni+Srwf3KK8HyZB9V4809q9TWE=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://coderlemon17.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://coderlemon17.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://coderlemon17.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://coderlemon17.github.io/apple-touch-icon.png><link rel=mask-icon href=https://coderlemon17.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.1/es5/tex-mml-chtml.min.js></script>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="详解Markov Chain Monte Carlo (MCMC): 从拒绝-接受采样到Gibbs Sampling"><meta property="og:description" content="0. Main Takeaway Sec 1. 介绍了蒙特卡洛采样 (MC) 和拒绝-接受采样. Sec 2. 介绍了马尔可夫链(MC)和基于马尔可夫链的采样. Sec 3. 介绍了马尔可夫链蒙特卡洛采样(MCM"><meta property="og:type" content="article"><meta property="og:url" content="https://coderlemon17.github.io/posts/2022/05-11-mcmc/"><meta property="og:image" content="https://coderlemon17.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-05-11T20:30:33+08:00"><meta property="article:modified_time" content="2022-05-11T20:30:33+08:00"><meta property="og:site_name" content="Lemon's Blog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://coderlemon17.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="详解Markov Chain Monte Carlo (MCMC): 从拒绝-接受采样到Gibbs Sampling"><meta name=twitter:description content="0. Main Takeaway Sec 1. 介绍了蒙特卡洛采样 (MC) 和拒绝-接受采样. Sec 2. 介绍了马尔可夫链(MC)和基于马尔可夫链的采样. Sec 3. 介绍了马尔可夫链蒙特卡洛采样(MCM"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://coderlemon17.github.io/posts/"},{"@type":"ListItem","position":2,"name":"详解Markov Chain Monte Carlo (MCMC): 从拒绝-接受采样到Gibbs Sampling","item":"https://coderlemon17.github.io/posts/2022/05-11-mcmc/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"详解Markov Chain Monte Carlo (MCMC): 从拒绝-接受采样到Gibbs Sampling","name":"详解Markov Chain Monte Carlo (MCMC): 从拒绝-接受采样到Gibbs Sampling","description":"0. Main Takeaway Sec 1. 介绍了蒙特卡洛采样 (MC) 和拒绝-接受采样. Sec 2. 介绍了马尔可夫链(MC)和基于马尔可夫链的采样. Sec 3. 介绍了马尔可夫链蒙特卡洛采样(MCM","keywords":["math"],"articleBody":"0. Main Takeaway Sec 1. 介绍了蒙特卡洛采样 (MC) 和拒绝-接受采样. Sec 2. 介绍了马尔可夫链(MC)和基于马尔可夫链的采样. Sec 3. 介绍了马尔可夫链蒙特卡洛采样(MCMC)和它的一个改进版本Metropolis-Hastings (MH). Sec 4. 介绍了Gibbs采样, 它用来处理MCMC采样在高维空间低效的问题. Note:\n本文章主要基于刘建平的博客整理写成, 补充了一些细节. 1. Monte Carlo (MC) 采样 核心: $\\mathbb{E}_{x \\sim p(x)}[\\cdots] = \\frac{1}{n} \\sum\\limits_{i=1}^n[\\cdots]$. 故我们可以用数值采样的方法近似期望, 而期望又可以表示为求和/积分的形式, 故提供了一种使用数值采样计算求和/积分的方法.\nMonte Carlo是一类使用随机模拟去解决问题的方法的统称, 例如, 当我们希望估计某些不太好求解的积分问题:\nThis is my fig 我们可以只在$[a,b]$间随机选取一个点$x_0$, 然后将$f(x_0)*(b-a)$作为阴影部分面积的近似. 我们也可以更精细一点, 在$[a,b]$间随机采$N$个点$\\left \\{ x_i\\right \\}$, 然后将$(b-a) * \\frac{\\sum\\limits_{i=1}^N x_i}{n}$的值作为面积的近似. 我们可以这么做的理论依据是: 若$x\\sim U[a,b]$, 则$p(x) = \\frac{1}{b-a}$, 则:\n$$ \\int_x f(x) = \\int_x \\frac{f(x)}{p(x)} p(x) = \\mathbb{E}_{x\\sim p(x)}[\\frac{f(x)}{p(x)}] = \\mathbb{E}_{x\\sim p(x)}[(b-a) f(x)] \\approx \\frac{1}{n}\\sum_{i=1}^N (b-a) f(x_i) $$\n故我们实际上不用局限于均匀分布, 对于任意分布$p(x)$, 我们都可以使用$\\frac{1}{n}\\sum\\limits_{i=1}^N (b-a) f(x_i) , \\text{ where } x_i \\sim p(x)$来近似求解$\\int_x f(x)$.\n1.1. Box-Muller 即便我们知道概率密度函数$p(x)$, 如何从$p(x)$中采样仍然是一个non-trivial的问题 (特别是从高维空间中), 使用Probability Integral Theorem是一个选择, 但是它需要我们求出累计分布函数同时求逆, 这仍是十分困难的. 为此一些trick常常需要被使用.\nTheorem: Let $X$ be a continuous random variable with c.d.f. $F(x)$, then $F(X) \\sim \\mathcal{U}(0,1)$\nProof: Let $Y = F(X)$ and has a c.d.f. $\\mathcal{G}(y)$, then:\n$$ \\mathcal{G}(y) = P(Y \\leq y) = P(F(X) \\leq y) = P(x \\leq F^{-1}(y)) = F(F^{-1}(y)) = y $$\nThen: Let $U \\sim \\mathcal{U}(0,1)$, given an arbitrary distribution with c.d.f. $F(x)$, then assign $X = F^{-1}(U)$, $X$ will have the distribution $F(x)$.\nBox-Muller变换:\nInput: $U_1,U_2 \\sim U[0,1]$\nOutput: $X,Y \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$\n$$ X = \\cos(2\\pi U_1)\\sqrt{-2\\ln U_2}\\\\ Y = \\sin(2\\pi U_1)\\sqrt{-2\\ln U_2} $$\nProof:\n假设$X,Y \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$, 则$p(x,y) = \\frac{1}{2\\pi} \\exp(- \\frac{x^2 + y^2}{2})$, 若我们记$X=R\\cos \\Theta, Y = R \\sin \\Theta$, 由概率密度换元可得:\n$$ p(r,\\theta) = \\frac{1}{2\\pi}\\exp (-\\frac{r^2}{2})\\lvert\\det \\frac{\\partial (x,y)}{\\partial (r,\\theta)} \\rvert = \\frac{r}{2\\pi}\\exp (-\\frac{r^2}{2}) $$\n则$p(\\theta) = \\int_0^\\infty p(r,\\theta)\\mathrm{d}r = \\frac{1}{2\\pi}$, 可知$\\Theta \\sim U[0,2\\pi]$.\n而$F(R\\leq r) = \\int_0^r \\int_0^{2\\pi}p(r,\\theta) \\mathrm{d}\\theta \\mathrm{d}r = 1 - \\exp (-\\frac{r^2}{2})$, 故由Probability Integral Theorem, $R= F^{-1}(U) =\\sqrt {-2\\ln (1-U)}$, 其中$U\\sim U[0,1]$.\n故我们如果取$U_1 = \\frac{\\Theta}{2\\pi}$, $U_2 = 1-U$, 则我们有$U_1, U_2 \\sim U[0,1]$, 且$X = R\\cos\\Theta =\\cos(2\\pi U_1)\\sqrt{-2\\ln U_2}, Y = R\\sin\\Theta = \\sin(2\\pi U_1)\\sqrt{-2\\ln U_2}$, 且$X,Y \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$.\n其他一些常见的连续分布, 比如t分布, F分布, Beta分布, Gamma分布等, 都可以通过类似的方式从$U[0,1]$得到的采样样本转化得到. 在python的numpy, scikit-learn等类库中, 都有生成这些常用分布样本的函数可以使用.\n1.2. Rejection Sampling 假设我们有一个简单易采样的分布$q(z)$, 我们希望从复杂的目标分布$\\tilde p(z)$中采样 (我们记归一化后的分布为$p(z) = \\frac{\\tilde p(z)}{\\int_z \\tilde p(z)} = \\frac{\\tilde p(z)}{Z_p}$).\n拒绝接受采样的工作过程如下:\n首先选取$k$, 使得$\\forall z, kq(z)\\geq \\tilde p(z)$.\nSample $z_0 \\sim q(z)$.\n以$\\frac{\\tilde p(z)}{kq(z)}$的概率接受这个样本, 否则拒绝.\nSample $u \\sim U[0,kq(z_0)]$, 若$u \u003e \\tilde p(z_0)$则拒绝这个样本 (落在图中灰色部分), 反之接受. 首先我们证明接受拒绝采样可以做到从$p(z)$中采样:\n我们记二值随机变量$A$, $A= 1$表示接受, 反之拒绝, $p(A=1) = \\int_z p(A=1|z) p(z) = \\int_z\\frac{\\tilde p(z)}{kq(z)}q(z)= \\frac{Z_p}{k}$则我们有:\n$$ p(z_0|A=1) = \\frac{p(A=1|z_0)p(z_0)}{p(A=1)} = \\frac{\\frac{\\tilde p(z_0)}{kq(z_0)}q(z_0)}{p(A=1)} = \\frac{\\tilde p(z_0)}{Z_p} = p(z_0) $$ 接下来我们讨论$k$应该如何取值:\n由$p(A=1)$可知, 接受率和$k$成反比, 故为了接受率更高, $k$应该越小越好. Summary\n作为MC方法中的一种, 接受-拒绝采样可以在一定程度上帮我们处理目标分布过于复杂难以采样的问题, 但是它仍然存在很多缺陷:\n对于某些多维分布, 条件概率分布$p(X_i|X_{-i})$可能容易得到, 但是联合分布$p(X)$很难求.\n对于高维度问题存在挑战:\n假设$X\\in R^{1000}$, $p(X) = \\mathcal{N}(0, \\Sigma_1), q(X) = \\mathcal{N}(0,\\Sigma_2)$, 且$\\Sigma_2 = 1.001 \\Sigma_1$. 则为了求$k$, 考虑零点处$\\frac{p(0)}{q(0)} = \\sqrt \\frac{|\\Sigma_2|}{|\\Sigma_1|} = 1.001^{500}\\approx 496,984,196,731,226,689,628.694$ (Curse of dimensionality), 会导致接受率极低. 总的来说接受-拒绝采样只在$q(z)$能较好拟合$\\tilde p(z)$的情况下work的比较好.\n2. Markov Chain (MC) 2.1. Markov Chain and Stationary Distribution 虽然接下来的推导都是在离散情况下, 但是可以被扩展到连续情况下.\n对于随机序列$\\left \\{ X_i\\right \\}$, 如果对于任意$X_i$, 我们有\n$$ p(X_i| X_{i-1}, \\cdots, X_1) = p(X_i|X_{i-1}) $$\n我们称其为马尔可夫链. 同时我们记转移矩阵$P_{ij} = P(i,j) = p(x_j |x_i)$, 在$t$时刻$X_t$取$x_i$的概率为: $\\pi_{t}(i) = p(X_t = x_i)$.\n我们记录 $\\pi_t = \\left \\{ \\pi_t(x_1), \\cdots, \\pi_t(x_i)\\cdots\\right \\}$为当前链条在$t$时刻的概率分布, 则我们易得$\\pi_{t+1} = \\pi_t P$. 我们记当前马氏链的极限分布为$\\lim\\limits_{t\\rightarrow\\infty}\\pi_t = \\pi^{*}$ (如果存在), 我们即当前马氏链的平稳分布为 $\\pi$ (如果存在):\n$$ \\pi = \\pi P $$\n由马氏链的性质我们可得:\n对于非周期遍历链, 马氏链的极限分布存在且等于平稳分布.\n非周期: 对于任意状态$x_i$, 记$d$为$\\left \\{ n|n\\geq 1, p^n_{ii}\u003e0\\right \\}$的最大公约数, 若$d=1$, 则该马氏链为非周期的 ($p^n_{ii}$是从$x_i$出发, 经过$n$步恰好回到$x_i$的概率). 直觉上理解, 对于周期马氏链会存在循环, 自然就不会有平稳分布. 遍历链: 正常返非周期链被称为遍历链条. 正常返指的是任意状态, 在无限长的马氏链中都会被访问无限次. 故遍历链可以简单认为就是任意状态之间两两可达. (不会有状态在$t\\rightarrow \\infty$的情况下被无法访问). 2.2. 基于Markov Chain采样 对于任意给定分布$\\pi$, 如果我们能找到对应的(非周期遍历)马氏链 (对应的状态转移矩阵$P$), 使得$\\pi$是该马氏链的平稳分布,那我们就可以通过这条马氏链采样得到服从分布$\\pi$的样本.\n假设$n$轮后马氏链收敛到平稳分布, 即:\n$$ \\pi_n = \\pi_{n+1} = \\cdots = \\pi $$\n则我们从$\\pi_n$开始采样就可以得到符合我们目标分布$\\pi$的样本了. 具体操作如下:\n输入马尔科夫链状态转移矩阵 $P$ , 设定状态转移次数阈值 $n_{1}$ , 需要的样本个数 $n_{2}$.\n从任意简单概率分布采样得到初始状态值 $x_{0}$\nfor $t=0$ to $n_{1}+n_{2}-1$ :\n从条件概率分布 $P\\left(x \\mid x_{t}\\right)$ 中采样得到样本 $x_{t+1}$ 样本集 $\\left(x_{n_{1}}, x_{n_{1}+1}, \\ldots, x_{n_{1}+n_{2}-1}\\right)$ 即为我们需要的平稳分布对应的样本集.\n3. Markov Chain Monte Carlo (MCMC) and Metropolis-Hastings (MH) 3.1. Detailed Balance 在基于MC采样中, 最大的问题是给定分布$\\pi$, 如何找到对应的马氏链 ($P$)使之满足$P$的平稳分布是$\\pi$. MCMC用一种巧妙的迂回的方式 (类似接受-拒绝采样) 解决了这个问题, 同时MH是一种提升MCMC采样效率的简单改进.\n在MCMC中, 主要解决的是, 给定分布$\\pi$, 如何找到对应的马氏链 ($P$)使之满足$P$的平稳分布是$\\pi$. 首先我们定义马氏链的细致平稳条件(detailed balance), 我们称$\\pi$满足细致平稳条件, 如果它满足:\n$$ \\forall x_i, x_j, \\pi(x_i)P(i,j) = x_j P(j,i) $$\n不难证明, 如果$\\pi$满足了细致平稳条件, 则它是马氏链的平稳分布:\n$$ \\sum_{i=1}^{\\infty} \\pi(x_i) P(i, j)=\\sum_{i=1}^{\\infty} \\pi(x_j) P(j, i)=\\pi(x_j) \\sum_{i=1}^{\\infty} P(j, i)=\\pi(x_j) $$\n即:\n$$ \\pi = \\pi P $$\n因此现在问题变成了, 给定分布$\\pi$, 如何寻找转移方程$Q$, 满足 $\\pi(x_i)Q(i,j) = \\pi(x_j) Q(j,i)$.\n3.2. MCMC 对于一般的$Q(i,j)$, 细致平稳条件是不满足的:\n$$ \\pi(x_i)Q(i,j) \\neq \\pi(x_j) Q(j,i) $$\n如果我们对上式做一个改造, 引入一个$\\alpha(i,j)$来使上式取等, 即:\n$$ \\pi(x_i)Q(i,j)\\alpha(i,j) = \\pi(x_j) Q(j,i)\\alpha(j,i)\\\\ \\text{where } \\alpha(i,j) = \\pi(j) Q(j,i) $$\n这样我们就得到了满足细致平稳条件的转移矩阵$P$, 满足:\n$$ P(i,j) =Q(i,j)\\alpha(i,j) $$\n同时$P$对应的平稳分布就是目标分布$\\pi$.\n这里我们的$P$是由某一个马氏链转移矩阵$Q(i,j)$乘以$\\alpha(i,j)$得到, 其中$\\alpha(i,j)$我们一般称为接受率, 它的取值在$[0,1]$之间, 可以理解为是一个概率值, 代表了我们接受转移的概率. 它和第1节中提到的拒绝-接受采样的思路类似, 前者是通过拒绝-接受概率拟合一个复杂分布, 这里是通过拒绝-接受概率得到一个满足细致平稳条件的转移矩阵.\n注意, 对于$P(i,j) =Q(i,j)\\alpha(i,j)$, $\\sum\\limits_{j} P(i,j)$不一定等于$1$, 这是正常的, 因为我们在使用MCMC做采样的时候, 下一步采样是有概率被拒绝的, 所以你可以在马氏链中额外加一个指向自己的自环, 概率值是$1- \\sum\\limits_{j} P(i,j)$.\n我们得到MCMC的采样过程:\n输入我们任意选定的马尔科夫链状态转移矩阵 $Q$ , 平稳分布 $\\pi(x)$ , 设定状态转移次数阈值 $n_{1}$ , 需要的样本个数 $n_{2}$\n从任意简单概率分布采样得到初始状态值 $x_{0}$.\nfor $t=0$ to $n_{1}+n_{2}-1$ :\n从条件概率分布 $Q\\left(x \\mid x_{t}\\right)$ 中采样得到样本 $x_{*}$.\n以概率$\\alpha\\left(x_{t}, x_{*}\\right)$接受样本$x_{*}$:\n从均匀分布采样 $u\\sim U [0,1]$ 如果 $u\u003c\\alpha\\left(x_{t}, x_{*}\\right)$, $=\\pi\\left(x_{*}\\right)$, $Q\\left(x_{*}, x_{t}\\right)$ 则接受转移 $x_{t} \\rightarrow x_{*}$ , 即 $x_{t+1}=x_{*}$ 否则不接受转移, 即 $x_{t+1}=x_{t}$ 样本集 $\\left(x_{n_{1}}, x_{n_{1}+1}, \\ldots, x_{n_{1}+n_{2}-1}\\right)$ 即为我们需要的平稳分布对应的样本集。\n3.3. Metropolis-Hastings Sampling Metropolis-Hastings采样简称M-H采样，这个算法首先由Metropolis提出，被Hastings改进，因此被称之为Metropolis-Hastings采样.\n在上述MCMC采样中, 一个问题是, $\\alpha(i,j)$作为接受概率, 如果$\\alpha(i,j)$过低, 会导致MCMC采样的效率很低下. 回顾平稳条件:\n$$ \\pi(x_i)Q(i,j)\\alpha(i,j) = \\pi(x_j) Q(j,i)\\alpha(j,i) $$\n采样低下的根本原因是$\\alpha(i,j)$太小了, 我们假设有:\n$$ \\pi(x_i)Q(i,j)\\times 0.1 = \\pi(x_j) Q(j,i)\\times 0.2 $$\n如果我们能同时把等式两边扩大$5$倍, 可以看到细致平稳条件仍然满足:\n$$ \\pi(x_i)Q(i,j)\\times 0.5 = \\pi(x_j) Q(j,i)\\times 1 $$\n因此如果我们令 $\\alpha(i,j) = \\min\\left \\{ \\frac{\\alpha(i,j)}{\\alpha(j,i)}, 1\\right \\} = \\min\\left \\{ \\frac{\\pi(x_j)Q(j,i)}{\\pi(x_i)Q(i,j)},1\\right \\}$, 就得到了M-H采样, 同时易证细致平稳条件仍然满足:\n$$ \\pi(x_i)Q(i,j)\\times \\min\\left \\{ \\frac{\\alpha(i,j)}{\\alpha(j,i)}, 1\\right \\}= \\pi(x_j) Q(j,i)\\times\\min\\left \\{ \\frac{\\alpha(j,i)}{\\alpha(i,j)}, 1\\right \\} $$\n由此M-H采样过程为:\n输入我们任意选定的马尔科夫链状态转移矩阵 $Q$ , 平稳分布 $\\pi(x)$ , 设定状态转移次数阈值 $n_{1}$ , 需要的样本个数 $n_{2}$\n从任意简单概率分布采样得到初始状态值 $x_{0}$.\nfor $t=0$ to $n_{1}+n_{2}-1$ :\n从条件概率分布 $Q\\left(x \\mid x_{t}\\right)$ 中采样得到样本 $x_{*}$.\n以概率$\\min\\left \\{ \\frac{\\alpha(x_t,x^{*})}{\\alpha(x^{*},x_t)}, 1\\right \\}$接受样本$x_{*}$:\n从均匀分布采样 $u\\sim U [0,1]$ 如果 $u\u003c \\min\\left \\{ \\frac{\\alpha(x_t,x^{*})}{\\alpha(x^{*},x_t)}, 1\\right \\}$ , 则接受转移 $x_{t} \\rightarrow x_{*}$ , 即 $x_{t+1}=x_{*}$ 否则不接受转移, 即 $x_{t+1}=x_{t}$ 如果我们选择的转移概率矩阵$Q$是对称的, 即满足$Q(i,j)=Q(j,i)$, 此时$\\alpha(i,j)$的表示可以进一步简化为:\n$$ \\alpha(i,j) = \\min\\left \\{ \\frac{\\pi(x_j)}{\\pi(x_i)},1\\right \\} $$\n在PRML中指出, 通过修改接受率$\\alpha(i,j)$, M-H算法能在很大程度上提升MCMC的采样效率.\n4. Gibbs Sampling 在第3节中, MCMC (M-H)算法已经可以做到从任意样本中采样的问题, 但是它仍然存在两个问题:\n是需要计算接受率, 在高维时计算量大, 并且由于接受率的原因导致算法收敛时间变长. 对于高维数据, 往往数据的条件概率分布易得, 而联合概率分布不易得. 4.1. 重新寻找细致平稳条件 在3.2. MCMC中, 我们通过引入接受概率$\\alpha(i,j)$来满足平稳分布条件, 这里我们尝试换一个思路.\n考虑二维随机变量$(X,Y)$, 我们假设它们服从联合分布$\\pi(x,y)$. 我们考虑两个第一维特征相同的两个点$A(x^1, y^1)$和$B(x^1, y^2)$, 我们不难发现:\n$$ \\pi(x^1, y^1) \\pi(y^2|x^1) = \\pi(y^1|x^1)\\pi(x^1)\\pi(y^2|x^1)\\\\ \\pi(x^1, y^2) \\pi(y^1|x^1)= \\pi(y^2|x^1)\\pi(x^1)\\pi(y^1|x^1) $$\n即$\\pi(x^1, y^1) \\pi(y^2|x^1) = \\pi(x^1, y^2) \\pi(y^1|x^1)$.\n换言之, 在$X=x^1$这条直线上, 如果我们取$\\pi(y^2|x^1)$作为从$A$到$B$的状态转移概率$P(A,B)$, 则在这条直线上的任意两点满足细致平稳条件. (保持$Y= y^1$可以得到类似的结果)\n基于这个发现, 我们可以构造满足细致平稳条件的状态转移矩阵$P$:\n$$ \\left \\{ \\begin{aligned} P(A,B) \u0026= \\pi(Y^{(B)}|X), \\text{ if }X^{(A)} = X^{(B)} =X\\\\ P(A,C) \u0026= \\pi(X^{(C)}|Y), \\text{ if }Y^{(A)} = Y^{(C)} =Y\\\\ P(A,D) \u0026= 0, \\text{ otherwise} \\end{aligned} \\right . $$\n由此我们得到了$P$, 它对应的平稳分布就是$\\pi(x,y)$, 我们给出二维Gibbs采样的流程:\n输入平稳分布 $\\pi\\left(x, y\\right)$ , 设定状态转移次数阈值 $n_{1}$ , 需要的样本个数 $n_{2}$. 随机初始化初始状态值 $x^{(0)}$ 和 $y^{(0)}$ for $t=0$ to $n_{1}+n_{2}-1$ : 从条件概率分布 $P\\left(y \\mid x^{(t)}\\right)$ 中采样得到样本 $y^{t+1}$ 从条件概率分布 $P\\left(x \\mid y^{(t+1)}\\right)$ 中采样得到样本 $x^{t+1}$ 样本集 $\\left\\{\\left(x^{\\left(n_{1}\\right)}, y^{\\left(n_{1}\\right)}\\right),\\left(x^{\\left(n_{1}+1\\right)}, y^{\\left(n_{1}+1\\right)}\\right), \\ldots,\\left(x^{\\left(n_{1}+n_{2}-1\\right)},y^{\\left(n_{1}+n_{2}-1\\right)}\\right)\\right\\}$ 即为我们需要的平稳分布对应的样本集.\n整个采样过程中，我们通过轮换坐标轴，采样的过程为:\n$$ \\left(x_{1}^{(1)}, x_{2}^{(1)}\\right) \\rightarrow\\left(x_{1}^{(1)}, x_{2}^{(2)}\\right) \\rightarrow\\left(x_{1}^{(2)}, x_{2}^{(2)}\\right) \\rightarrow \\ldots \\rightarrow\\left(x_{1}^{\\left(n_{1}+n_{2}-1\\right)}, x_{2}^{\\left(n_{1}+n_{2}-1\\right)}\\right) $$\n用下图可以很直观的看出, 采样是在两个坐标轴上不停的轮换的. (坐标轴轮换不是必须的, 我们也可以每次随机选择一个坐标轴进行采样. 不过常用的Gibbs采样的实现都是基于坐标轴轮换的.)\n4.2. 多维Gibbs采样 Gibbs采样至少需要两个维度 (1维可以使用M-H), 于是它可以被推广到高维. 在高维度中, 我们选取的马氏链的状态转移概率为$\\pi(x_i|x_1,x_2,…,x_{i-1},x_{i+1},…,x_n) = \\pi(x_i|x_{-i})$, 即我们固定$n-1$个坐标轴, 在剩余的一个坐标轴上进行移动(采样). 具体采样过程和上述二维中的类似.\nReferences ​MCMC: 刘建平 Pattern Recognition and Machine Learning 《应用随机过程》(林元烈) ","wordCount":"5095","inLanguage":"en","datePublished":"2022-05-11T20:30:33+08:00","dateModified":"2022-05-11T20:30:33+08:00","author":{"@type":"Person","name":"Xinning Zhou"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://coderlemon17.github.io/posts/2022/05-11-mcmc/"},"publisher":{"@type":"Organization","name":"Lemon's Blog","logo":{"@type":"ImageObject","url":"https://coderlemon17.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://coderlemon17.github.io/ accesskey=h title="Lemon's Blog (Alt + H)">Lemon's Blog</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://coderlemon17.github.io/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://coderlemon17.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://ml.cs.tsinghua.edu.cn/ title=TSAIL><span>TSAIL</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://coderlemon17.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://coderlemon17.github.io/posts/>Posts</a></div><h1 class=post-title>详解Markov Chain Monte Carlo (MCMC): 从拒绝-接受采样到Gibbs Sampling</h1><div class=post-meta><span title='2022-05-11 20:30:33 +0800 +0800'>May 11, 2022</span>&nbsp;·&nbsp;11 min&nbsp;·&nbsp;Xinning Zhou&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/posts/2022/05-11-MCMC/index.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#0-main-takeaway aria-label="0. Main Takeaway">0. Main Takeaway</a></li><li><a href=#1-monte-carlo-mc-%e9%87%87%e6%a0%b7 aria-label="1. Monte Carlo (MC) 采样">1. Monte Carlo (MC) 采样</a><ul><li><a href=#11-box-muller aria-label="1.1. Box-Muller">1.1. Box-Muller</a></li><li><a href=#12-rejection-sampling aria-label="1.2. Rejection Sampling">1.2. Rejection Sampling</a></li></ul></li><li><a href=#2-markov-chain-mc aria-label="2. Markov Chain (MC)">2. Markov Chain (MC)</a><ul><li><a href=#21-markov-chain-and-stationary-distribution aria-label="2.1. Markov Chain and Stationary Distribution">2.1. Markov Chain and Stationary Distribution</a></li><li><a href=#22-%e5%9f%ba%e4%ba%8emarkov-chain%e9%87%87%e6%a0%b7 aria-label="2.2. 基于Markov Chain采样">2.2. 基于Markov Chain采样</a></li></ul></li><li><a href=#3-markov-chain-monte-carlo-mcmc-and-metropolis-hastings-mh aria-label="3. Markov Chain Monte Carlo (MCMC) and Metropolis-Hastings (MH)">3. Markov Chain Monte Carlo (MCMC) and Metropolis-Hastings (MH)</a><ul><li><a href=#31-detailed-balance aria-label="3.1. Detailed Balance">3.1. Detailed Balance</a></li><li><a href=#32-mcmc aria-label="3.2. MCMC">3.2. MCMC</a></li><li><a href=#33-metropolis-hastings-sampling aria-label="3.3. Metropolis-Hastings Sampling">3.3. Metropolis-Hastings Sampling</a></li></ul></li><li><a href=#4-gibbs-sampling aria-label="4. Gibbs Sampling">4. Gibbs Sampling</a><ul><li><a href=#41-%e9%87%8d%e6%96%b0%e5%af%bb%e6%89%be%e7%bb%86%e8%87%b4%e5%b9%b3%e7%a8%b3%e6%9d%a1%e4%bb%b6 aria-label="4.1. 重新寻找细致平稳条件">4.1. 重新寻找细致平稳条件</a></li><li><a href=#42-%e5%a4%9a%e7%bb%b4gibbs%e9%87%87%e6%a0%b7 aria-label="4.2. 多维Gibbs采样">4.2. 多维Gibbs采样</a></li></ul></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><h1 id=0-main-takeaway>0. Main Takeaway<a hidden class=anchor aria-hidden=true href=#0-main-takeaway>#</a></h1><ul><li>Sec 1. 介绍了蒙特卡洛采样 (MC) 和拒绝-接受采样.</li><li>Sec 2. 介绍了马尔可夫链(MC)和基于马尔可夫链的采样.</li><li>Sec 3. 介绍了马尔可夫链蒙特卡洛采样(MCMC)和它的一个改进版本Metropolis-Hastings (MH).</li><li>Sec 4. 介绍了Gibbs采样, 它用来处理MCMC采样在高维空间低效的问题.</li></ul><p><code>Note</code>:</p><ul><li>本文章主要基于<a href=https://www.cnblogs.com/pinard/p/6625739.html>刘建平的博客</a>整理写成, 补充了一些细节.</li></ul><h1 id=1-monte-carlo-mc-采样>1. Monte Carlo (MC) 采样<a hidden class=anchor aria-hidden=true href=#1-monte-carlo-mc-采样>#</a></h1><blockquote><p>核心: $\mathbb{E}_{x \sim p(x)}[\cdots] = \frac{1}{n} \sum\limits_{i=1}^n[\cdots]$. 故我们可以用数值采样的方法近似期望, 而期望又可以表示为求和/积分的形式, 故提供了一种使用数值采样计算求和/积分的方法.</p></blockquote><p>Monte Carlo是一类使用随机模拟去解决问题的方法的统称, 例如, 当我们希望估计某些不太好求解的积分问题:</p><figure class=align-center><img loading=lazy src=assets/image-20220511150844-6l62vdb.png#center width=50%><figcaption style=text-align:center;color:gray;font-weight:400>This is my fig</figcaption></figure><p>我们可以只在$[a,b]$间随机选取一个点$x_0$, 然后将$f(x_0)*(b-a)$作为阴影部分面积的近似. 我们也可以更精细一点, 在$[a,b]$间随机采$N$个点$\left \{ x_i\right \}$, 然后将$(b-a) * \frac{\sum\limits_{i=1}^N x_i}{n}$的值作为面积的近似. 我们可以这么做的理论依据是: 若$x\sim U[a,b]$, 则$p(x) = \frac{1}{b-a}$, 则:</p><p>$$
\int_x f(x) = \int_x \frac{f(x)}{p(x)} p(x) = \mathbb{E}_{x\sim p(x)}[\frac{f(x)}{p(x)}] = \mathbb{E}_{x\sim p(x)}[(b-a) f(x)] \approx \frac{1}{n}\sum_{i=1}^N (b-a) f(x_i)
$$</p><p>故我们实际上不用局限于均匀分布, 对于任意分布$p(x)$, 我们都可以使用$\frac{1}{n}\sum\limits_{i=1}^N (b-a) f(x_i) , \text{ where } x_i \sim p(x)$来近似求解$\int_x f(x)$.</p><h2 id=11-box-muller>1.1. Box-Muller<a hidden class=anchor aria-hidden=true href=#11-box-muller>#</a></h2><p>即便我们知道概率密度函数$p(x)$, 如何从$p(x)$中采样仍然是一个non-trivial的问题 (特别是从高维空间中), 使用Probability Integral Theorem是一个选择, 但是它需要我们求出累计分布函数同时求逆, 这仍是十分困难的. 为此一些trick常常需要被使用.</p><blockquote><p>Theorem: Let $X$ be a continuous random variable with c.d.f. $F(x)$, then $F(X) \sim \mathcal{U}(0,1)$</p><p>Proof: Let $Y = F(X)$ and has a c.d.f. $\mathcal{G}(y)$, then:</p><p>$$
\mathcal{G}(y) = P(Y \leq y) = P(F(X) \leq y) = P(x \leq F^{-1}(y)) = F(F^{-1}(y)) = y
$$</p><p>Then: Let $U \sim \mathcal{U}(0,1)$, given an arbitrary distribution with c.d.f. $F(x)$, then assign $X = F^{-1}(U)$, $X$ will have the distribution $F(x)$.</p></blockquote><p><strong>Box-Muller变换:</strong></p><p>Input: $U_1,U_2 \sim U[0,1]$</p><p>Output: $X,Y \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$</p><p>$$
X = \cos(2\pi U_1)\sqrt{-2\ln U_2}\\
Y = \sin(2\pi U_1)\sqrt{-2\ln U_2}
$$</p><p><em>Proof:</em></p><p>假设$X,Y \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$, 则$p(x,y) = \frac{1}{2\pi} \exp(- \frac{x^2 + y^2}{2})$, 若我们记$X=R\cos \Theta, Y = R \sin \Theta$, 由概率密度换元可得:</p><p>$$
p(r,\theta) = \frac{1}{2\pi}\exp (-\frac{r^2}{2})\lvert\det \frac{\partial (x,y)}{\partial (r,\theta)} \rvert = \frac{r}{2\pi}\exp (-\frac{r^2}{2})
$$</p><p>则$p(\theta) = \int_0^\infty p(r,\theta)\mathrm{d}r = \frac{1}{2\pi}$, 可知$\Theta \sim U[0,2\pi]$.</p><p>而$F(R\leq r) = \int_0^r \int_0^{2\pi}p(r,\theta) \mathrm{d}\theta \mathrm{d}r = 1 - \exp (-\frac{r^2}{2})$, 故由Probability Integral Theorem, $R= F^{-1}(U) =\sqrt {-2\ln (1-U)}$, 其中$U\sim U[0,1]$.</p><hr><p>故我们如果取$U_1 = \frac{\Theta}{2\pi}$, $U_2 = 1-U$, 则我们有$U_1, U_2 \sim U[0,1]$, 且$X = R\cos\Theta =\cos(2\pi U_1)\sqrt{-2\ln U_2}, Y = R\sin\Theta = \sin(2\pi U_1)\sqrt{-2\ln U_2}$, 且$X,Y \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$.</p><p>其他一些常见的连续分布, 比如t分布, F分布, Beta分布, Gamma分布等, 都可以通过类似的方式从$U[0,1]$得到的采样样本转化得到. 在python的numpy, scikit-learn等类库中, 都有生成这些常用分布样本的函数可以使用.</p><h2 id=12-rejection-sampling>1.2. Rejection Sampling<a hidden class=anchor aria-hidden=true href=#12-rejection-sampling>#</a></h2><p>假设我们有一个简单易采样的分布$q(z)$, 我们希望从复杂的目标分布$\tilde p(z)$中采样 (我们记归一化后的分布为$p(z) = \frac{\tilde p(z)}{\int_z \tilde p(z)} = \frac{\tilde p(z)}{Z_p}$).</p><figure class=align-center><img loading=lazy src=assets/image-20220511160858-8680bq4.png#center width=50%></figure><p>拒绝接受采样的工作过程如下:</p><p>首先选取$k$, 使得$\forall z, kq(z)\geq \tilde p(z)$.</p><ol><li><p>Sample $z_0 \sim q(z)$.</p></li><li><p>以$\frac{\tilde p(z)}{kq(z)}$的概率接受这个样本, 否则拒绝.</p><ul><li>Sample $u \sim U[0,kq(z_0)]$, 若$u > \tilde p(z_0)$则拒绝这个样本 (落在图中灰色部分), 反之接受.</li></ul></li></ol><p>首先我们证明接受拒绝采样可以做到从$p(z)$中采样:</p><ul><li><p>我们记二值随机变量$A$, $A= 1$表示接受, 反之拒绝, $p(A=1) = \int_z p(A=1|z) p(z) = \int_z\frac{\tilde p(z)}{kq(z)}q(z)= \frac{Z_p}{k}$则我们有:</p><ul><li>$$
p(z_0|A=1) = \frac{p(A=1|z_0)p(z_0)}{p(A=1)} = \frac{\frac{\tilde p(z_0)}{kq(z_0)}q(z_0)}{p(A=1)} = \frac{\tilde p(z_0)}{Z_p} = p(z_0)
$$</li></ul></li></ul><p>接下来我们讨论$k$应该如何取值:</p><ul><li>由$p(A=1)$可知, 接受率和$k$成反比, 故为了接受率更高, $k$应该越小越好.</li></ul><p><strong>Summary</strong></p><ul><li><p>作为MC方法中的一种, 接受-拒绝采样可以在一定程度上帮我们处理目标分布过于复杂难以采样的问题, 但是它仍然存在很多缺陷:</p><ul><li><p>对于某些多维分布, 条件概率分布$p(X_i|X_{-i})$可能容易得到, 但是联合分布$p(X)$很难求.</p></li><li><p>对于高维度问题存在挑战:</p><ul><li>假设$X\in R^{1000}$, $p(X) = \mathcal{N}(0, \Sigma_1), q(X) = \mathcal{N}(0,\Sigma_2)$, 且$\Sigma_2 = 1.001 \Sigma_1$.</li><li>则为了求$k$, 考虑零点处$\frac{p(0)}{q(0)} = \sqrt \frac{|\Sigma_2|}{|\Sigma_1|} = 1.001^{500}\approx 496,984,196,731,226,689,628.694$ (Curse of dimensionality), 会导致接受率极低.</li></ul></li><li><p>总的来说接受-拒绝采样只在$q(z)$能较好拟合$\tilde p(z)$的情况下work的比较好.</p></li></ul></li></ul><h1 id=2-markov-chain-mc>2. Markov Chain (MC)<a hidden class=anchor aria-hidden=true href=#2-markov-chain-mc>#</a></h1><h2 id=21-markov-chain-and-stationary-distribution>2.1. Markov Chain and Stationary Distribution<a hidden class=anchor aria-hidden=true href=#21-markov-chain-and-stationary-distribution>#</a></h2><blockquote><p>虽然接下来的推导都是在离散情况下, 但是可以被扩展到连续情况下.</p></blockquote><figure class=align-center><img loading=lazy src=assets/image-20220511164728-lkhhqq4.png#center width=50%></figure><p>对于随机序列$\left \{ X_i\right \}$, 如果对于任意$X_i$, 我们有</p><p>$$
p(X_i| X_{i-1}, \cdots, X_1) = p(X_i|X_{i-1})
$$</p><p>我们称其为马尔可夫链. 同时我们记转移矩阵$P_{ij} = P(i,j) = p(x_j |x_i)$, 在$t$时刻$X_t$取$x_i$的概率为: $\pi_{t}(i) = p(X_t = x_i)$.</p><p>我们记录 $\pi_t = \left \{ \pi_t(x_1), \cdots, \pi_t(x_i)\cdots\right \}$为当前链条在$t$时刻的概率分布, 则我们易得$\pi_{t+1} = \pi_t P$. 我们记当前马氏链的极限分布为$\lim\limits_{t\rightarrow\infty}\pi_t = \pi^{*}$ (如果存在), 我们即当前马氏链的平稳分布为 $\pi$ (如果存在):</p><p>$$
\pi = \pi P
$$</p><p>由马氏链的性质我们可得:</p><p>对于非周期遍历链, 马氏链的极限分布存在且等于平稳分布.</p><ul><li>非周期: 对于任意状态$x_i$, 记$d$为$\left \{ n|n\geq 1, p^n_{ii}>0\right \}$的最大公约数, 若$d=1$, 则该马氏链为非周期的 ($p^n_{ii}$是从$x_i$出发, 经过$n$步恰好回到$x_i$的概率). 直觉上理解, 对于周期马氏链会存在循环, 自然就不会有平稳分布.</li><li>遍历链: 正常返非周期链被称为遍历链条. 正常返指的是任意状态, 在无限长的马氏链中都会被访问无限次. 故遍历链可以简单认为就是任意状态之间两两可达. (不会有状态在$t\rightarrow \infty$的情况下被无法访问).</li></ul><h2 id=22-基于markov-chain采样>2.2. 基于Markov Chain采样<a hidden class=anchor aria-hidden=true href=#22-基于markov-chain采样>#</a></h2><p>对于任意给定分布$\pi$, 如果我们能找到对应的(非周期遍历)马氏链 (对应的状态转移矩阵$P$), 使得$\pi$是该马氏链的平稳分布,<strong>那我们就可以通过这条马氏链采样得到服从分布</strong>$\pi$的样本.</p><p>假设$n$轮后马氏链收敛到平稳分布, 即:</p><p>$$
\pi_n = \pi_{n+1} = \cdots = \pi
$$</p><p>则我们从$\pi_n$开始采样就可以得到符合我们目标分布$\pi$的样本了. 具体操作如下:</p><ol><li><p>输入马尔科夫链状态转移矩阵 $P$ , 设定状态转移次数阈值 $n_{1}$ , 需要的样本个数 $n_{2}$.</p></li><li><p>从任意简单概率分布采样得到初始状态值 $x_{0}$</p></li><li><p>for $t=0$ to $n_{1}+n_{2}-1$ :</p><ol><li>从条件概率分布 $P\left(x \mid x_{t}\right)$ 中采样得到样本 $x_{t+1}$</li></ol></li></ol><p>样本集 $\left(x_{n_{1}}, x_{n_{1}+1}, \ldots, x_{n_{1}+n_{2}-1}\right)$ 即为我们需要的平稳分布对应的样本集.</p><h1 id=3-markov-chain-monte-carlo-mcmc-and-metropolis-hastings-mh>3. Markov Chain Monte Carlo (MCMC) and Metropolis-Hastings (MH)<a hidden class=anchor aria-hidden=true href=#3-markov-chain-monte-carlo-mcmc-and-metropolis-hastings-mh>#</a></h1><h2 id=31-detailed-balance>3.1. Detailed Balance<a hidden class=anchor aria-hidden=true href=#31-detailed-balance>#</a></h2><blockquote><p>在基于MC采样中, 最大的问题是给定分布$\pi$, 如何找到对应的马氏链 ($P$)使之满足$P$的平稳分布是$\pi$. MCMC用一种巧妙的迂回的方式 (类似接受-拒绝采样) 解决了这个问题, 同时MH是一种提升MCMC采样效率的简单改进.</p></blockquote><p>在MCMC中, 主要解决的是, 给定分布$\pi$, 如何找到对应的马氏链 ($P$)使之满足$P$的平稳分布是$\pi$. 首先我们定义马氏链的细致平稳条件(detailed balance), 我们称$\pi$满足细致平稳条件, 如果它满足:</p><p>$$
\forall x_i, x_j, \pi(x_i)P(i,j) = x_j P(j,i)
$$</p><p>不难证明, 如果$\pi$满足了细致平稳条件, 则它是马氏链的平稳分布:</p><p>$$
\sum_{i=1}^{\infty} \pi(x_i) P(i, j)=\sum_{i=1}^{\infty} \pi(x_j) P(j, i)=\pi(x_j) \sum_{i=1}^{\infty} P(j, i)=\pi(x_j)
$$</p><p>即:</p><p>$$
\pi = \pi P
$$</p><p>因此现在问题变成了, 给定分布$\pi$, 如何寻找转移方程$Q$, 满足 $\pi(x_i)Q(i,j) = \pi(x_j) Q(j,i)$.</p><h2 id=32-mcmc>3.2. MCMC<a hidden class=anchor aria-hidden=true href=#32-mcmc>#</a></h2><p>对于一般的$Q(i,j)$, 细致平稳条件是不满足的:</p><p>$$
\pi(x_i)Q(i,j) \neq \pi(x_j) Q(j,i)
$$</p><p>如果我们对上式做一个改造, 引入一个$\alpha(i,j)$来使上式取等, 即:</p><p>$$
\pi(x_i)Q(i,j)\alpha(i,j) = \pi(x_j) Q(j,i)\alpha(j,i)\\
\text{where } \alpha(i,j) = \pi(j) Q(j,i)
$$</p><p>这样我们就得到了满足细致平稳条件的转移矩阵$P$, 满足:</p><p>$$
P(i,j) =Q(i,j)\alpha(i,j)
$$</p><p>同时$P$对应的平稳分布就是目标分布$\pi$.</p><p>这里我们的$P$是由某一个马氏链转移矩阵$Q(i,j)$乘以$\alpha(i,j)$得到, 其中$\alpha(i,j)$我们一般称为接受率, 它的取值在$[0,1]$之间, 可以理解为是一个概率值, 代表了我们接受转移的概率. 它和第1节中提到的拒绝-接受采样的思路类似, 前者是通过拒绝-接受概率拟合一个复杂分布, 这里是通过拒绝-接受概率得到一个满足细致平稳条件的转移矩阵.</p><p>注意, 对于$P(i,j) =Q(i,j)\alpha(i,j)$, $\sum\limits_{j} P(i,j)$不一定等于$1$, 这是正常的, 因为我们在使用MCMC做采样的时候, 下一步采样是有概率被拒绝的, 所以你可以在马氏链中额外加一个指向自己的自环, 概率值是$1- \sum\limits_{j} P(i,j)$.</p><p>我们得到MCMC的采样过程:</p><ol><li><p>输入我们任意选定的马尔科夫链状态转移矩阵 $Q$ , 平稳分布 $\pi(x)$ , 设定状态转移次数阈值 $n_{1}$ , 需要的样本个数 $n_{2}$</p></li><li><p>从任意简单概率分布采样得到初始状态值 $x_{0}$.</p></li><li><p>for $t=0$ to $n_{1}+n_{2}-1$ :</p><ol><li><p>从条件概率分布 $Q\left(x \mid x_{t}\right)$ 中采样得到样本 $x_{*}$.</p></li><li><p>以概率$\alpha\left(x_{t}, x_{*}\right)$接受样本$x_{*}$:</p><ol><li>从均匀分布采样 $u\sim U [0,1]$</li><li>如果 $u&lt;\alpha\left(x_{t}, x_{*}\right)$, $=\pi\left(x_{*}\right)$, $Q\left(x_{*}, x_{t}\right)$ 则接受转移 $x_{t} \rightarrow x_{*}$ , 即 $x_{t+1}=x_{*}$</li><li>否则不接受转移, 即 $x_{t+1}=x_{t}$</li></ol></li></ol></li></ol><p>样本集 $\left(x_{n_{1}}, x_{n_{1}+1}, \ldots, x_{n_{1}+n_{2}-1}\right)$ 即为我们需要的平稳分布对应的样本集。</p><h2 id=33-metropolis-hastings-sampling>3.3. Metropolis-Hastings Sampling<a hidden class=anchor aria-hidden=true href=#33-metropolis-hastings-sampling>#</a></h2><blockquote><p>Metropolis-Hastings采样简称M-H采样，这个算法首先由Metropolis提出，被Hastings改进，因此被称之为Metropolis-Hastings采样.</p></blockquote><p>在上述MCMC采样中, 一个问题是, $\alpha(i,j)$作为接受概率, 如果$\alpha(i,j)$过低, 会导致MCMC采样的效率很低下. 回顾平稳条件:</p><p>$$
\pi(x_i)Q(i,j)\alpha(i,j) = \pi(x_j) Q(j,i)\alpha(j,i)
$$</p><p>采样低下的根本原因是$\alpha(i,j)$太小了, 我们假设有:</p><p>$$
\pi(x_i)Q(i,j)\times 0.1 = \pi(x_j) Q(j,i)\times 0.2
$$</p><p>如果我们能同时把等式两边扩大$5$倍, 可以看到细致平稳条件仍然满足:</p><p>$$
\pi(x_i)Q(i,j)\times 0.5 = \pi(x_j) Q(j,i)\times 1
$$</p><p>因此如果我们令 $\alpha(i,j) = \min\left \{ \frac{\alpha(i,j)}{\alpha(j,i)}, 1\right \} = \min\left \{ \frac{\pi(x_j)Q(j,i)}{\pi(x_i)Q(i,j)},1\right \}$, 就得到了M-H采样, 同时易证细致平稳条件仍然满足:</p><p>$$
\pi(x_i)Q(i,j)\times \min\left \{ \frac{\alpha(i,j)}{\alpha(j,i)}, 1\right \}= \pi(x_j) Q(j,i)\times\min\left \{ \frac{\alpha(j,i)}{\alpha(i,j)}, 1\right \}
$$</p><p>由此M-H采样过程为:</p><ol><li><p>输入我们任意选定的马尔科夫链状态转移矩阵 $Q$ , 平稳分布 $\pi(x)$ , 设定状态转移次数阈值 $n_{1}$ , 需要的样本个数 $n_{2}$</p></li><li><p>从任意简单概率分布采样得到初始状态值 $x_{0}$.</p></li><li><p>for $t=0$ to $n_{1}+n_{2}-1$ :</p><ol><li><p>从条件概率分布 $Q\left(x \mid x_{t}\right)$ 中采样得到样本 $x_{*}$.</p></li><li><p>以概率$\min\left \{ \frac{\alpha(x_t,x^{*})}{\alpha(x^{*},x_t)}, 1\right \}$接受样本$x_{*}$:</p><ol><li>从均匀分布采样 $u\sim U [0,1]$</li><li>如果 $u&lt; \min\left \{ \frac{\alpha(x_t,x^{*})}{\alpha(x^{*},x_t)}, 1\right \}$ , 则接受转移 $x_{t} \rightarrow x_{*}$ , 即 $x_{t+1}=x_{*}$</li><li>否则不接受转移, 即 $x_{t+1}=x_{t}$</li></ol></li></ol></li></ol><p>如果我们选择的转移概率矩阵$Q$是对称的, 即满足$Q(i,j)=Q(j,i)$, 此时$\alpha(i,j)$的表示可以进一步简化为:</p><p>$$
\alpha(i,j) = \min\left \{ \frac{\pi(x_j)}{\pi(x_i)},1\right \}
$$</p><p>在PRML中指出, 通过修改接受率$\alpha(i,j)$, M-H算法能在很大程度上提升MCMC的采样效率.</p><h1 id=4-gibbs-sampling>4. Gibbs Sampling<a hidden class=anchor aria-hidden=true href=#4-gibbs-sampling>#</a></h1><p>在第3节中, MCMC (M-H)算法已经可以做到从任意样本中采样的问题, 但是它仍然存在两个问题:</p><ol><li>是需要计算接受率, 在高维时计算量大, 并且由于接受率的原因导致算法收敛时间变长.</li><li>对于高维数据, 往往数据的条件概率分布易得, 而联合概率分布不易得.</li></ol><h2 id=41-重新寻找细致平稳条件>4.1. 重新寻找细致平稳条件<a hidden class=anchor aria-hidden=true href=#41-重新寻找细致平稳条件>#</a></h2><p>在3.2. MCMC中, 我们通过引入接受概率$\alpha(i,j)$来满足平稳分布条件, 这里我们尝试换一个思路.</p><p>考虑二维随机变量$(X,Y)$, 我们假设它们服从联合分布$\pi(x,y)$. 我们考虑两个第一维特征相同的两个点$A(x^1, y^1)$和$B(x^1, y^2)$, 我们不难发现:</p><p>$$
\pi(x^1, y^1) \pi(y^2|x^1) = \pi(y^1|x^1)\pi(x^1)\pi(y^2|x^1)\\
\pi(x^1, y^2) \pi(y^1|x^1)= \pi(y^2|x^1)\pi(x^1)\pi(y^1|x^1)
$$</p><p>即$\pi(x^1, y^1) \pi(y^2|x^1) = \pi(x^1, y^2) \pi(y^1|x^1)$.</p><p>换言之, 在$X=x^1$这条直线上, 如果我们取$\pi(y^2|x^1)$作为从$A$到$B$的状态转移概率$P(A,B)$, 则在这条直线上的任意两点满足细致平稳条件. (保持$Y= y^1$可以得到类似的结果)</p><p>基于这个发现, 我们可以构造满足细致平稳条件的状态转移矩阵$P$:</p><p>$$
\left \{
\begin{aligned}
P(A,B) &= \pi(Y^{(B)}|X), \text{ if }X^{(A)} = X^{(B)} =X\\
P(A,C) &= \pi(X^{(C)}|Y), \text{ if }Y^{(A)} = Y^{(C)} =Y\\
P(A,D) &= 0, \text{ otherwise}
\end{aligned}
\right .
$$</p><p>由此我们得到了$P$, 它对应的平稳分布就是$\pi(x,y)$, 我们给出二维Gibbs采样的流程:</p><ol><li>输入平稳分布 $\pi\left(x, y\right)$ , 设定状态转移次数阈值 $n_{1}$ , 需要的样本个数 $n_{2}$.</li><li>随机初始化初始状态值 $x^{(0)}$ 和 $y^{(0)}$</li><li>for $t=0$ to $n_{1}+n_{2}-1$ :</li><li>从条件概率分布 $P\left(y \mid x^{(t)}\right)$ 中采样得到样本 $y^{t+1}$</li><li>从条件概率分布 $P\left(x \mid y^{(t+1)}\right)$ 中采样得到样本 $x^{t+1}$</li></ol><p>样本集 $\left\{\left(x^{\left(n_{1}\right)}, y^{\left(n_{1}\right)}\right),\left(x^{\left(n_{1}+1\right)}, y^{\left(n_{1}+1\right)}\right), \ldots,\left(x^{\left(n_{1}+n_{2}-1\right)},y^{\left(n_{1}+n_{2}-1\right)}\right)\right\}$ 即为我们需要的平稳分布对应的样本集.</p><p>整个采样过程中，我们通过轮换坐标轴，采样的过程为:</p><p>$$
\left(x_{1}^{(1)}, x_{2}^{(1)}\right) \rightarrow\left(x_{1}^{(1)}, x_{2}^{(2)}\right) \rightarrow\left(x_{1}^{(2)}, x_{2}^{(2)}\right) \rightarrow \ldots \rightarrow\left(x_{1}^{\left(n_{1}+n_{2}-1\right)}, x_{2}^{\left(n_{1}+n_{2}-1\right)}\right)
$$</p><p>用下图可以很直观的看出, 采样是在两个坐标轴上不停的轮换的. (坐标轴轮换不是必须的, 我们也可以每次随机选择一个坐标轴进行采样. 不过常用的Gibbs采样的实现都是基于坐标轴轮换的.)</p><p><img src=assets/image-20220511194535-z0el6d7.png#center alt=image.png></p><h2 id=42-多维gibbs采样>4.2. 多维Gibbs采样<a hidden class=anchor aria-hidden=true href=#42-多维gibbs采样>#</a></h2><p>Gibbs采样至少需要两个维度 (1维可以使用M-H), 于是它可以被推广到高维. 在高维度中, 我们选取的马氏链的状态转移概率为$\pi(x_i|x_1,x_2,&mldr;,x_{i-1},x_{i+1},&mldr;,x_n) = \pi(x_i|x_{-i})$, 即我们固定$n-1$个坐标轴, 在剩余的一个坐标轴上进行移动(采样). 具体采样过程和上述二维中的类似.</p><h1 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h1><ul><li><a href>​</a><a href=https://www.cnblogs.com/pinard/p/6625739.html>MCMC: 刘建平</a></li><li><a href=https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf>Pattern Recognition and Machine Learning</a></li><li><a href=https://item.jd.com/12497810.html>《应用随机过程》(林元烈)</a></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://coderlemon17.github.io/tags/math/>math</a></li></ul><nav class=paginav><a class=prev href=https://coderlemon17.github.io/posts/2022/07-16-ot/><span class=title>« Prev</span><br><span>Optimal Transport入门简述</span></a>
<a class=next href=https://coderlemon17.github.io/posts/2022/02-14-ddp/><span class=title>Next »</span><br><span>torch.nn.parallel.DistributedDataParallel: 快速上手</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share 详解Markov Chain Monte Carlo (MCMC): 从拒绝-接受采样到Gibbs Sampling on twitter" href="https://twitter.com/intent/tweet/?text=%e8%af%a6%e8%a7%a3Markov%20Chain%20Monte%20Carlo%20%28MCMC%29%3a%20%e4%bb%8e%e6%8b%92%e7%bb%9d-%e6%8e%a5%e5%8f%97%e9%87%87%e6%a0%b7%e5%88%b0Gibbs%20Sampling&url=https%3a%2f%2fcoderlemon17.github.io%2fposts%2f2022%2f05-11-mcmc%2f&hashtags=math"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 详解Markov Chain Monte Carlo (MCMC): 从拒绝-接受采样到Gibbs Sampling on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fcoderlemon17.github.io%2fposts%2f2022%2f05-11-mcmc%2f&title=%e8%af%a6%e8%a7%a3Markov%20Chain%20Monte%20Carlo%20%28MCMC%29%3a%20%e4%bb%8e%e6%8b%92%e7%bb%9d-%e6%8e%a5%e5%8f%97%e9%87%87%e6%a0%b7%e5%88%b0Gibbs%20Sampling&summary=%e8%af%a6%e8%a7%a3Markov%20Chain%20Monte%20Carlo%20%28MCMC%29%3a%20%e4%bb%8e%e6%8b%92%e7%bb%9d-%e6%8e%a5%e5%8f%97%e9%87%87%e6%a0%b7%e5%88%b0Gibbs%20Sampling&source=https%3a%2f%2fcoderlemon17.github.io%2fposts%2f2022%2f05-11-mcmc%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 详解Markov Chain Monte Carlo (MCMC): 从拒绝-接受采样到Gibbs Sampling on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fcoderlemon17.github.io%2fposts%2f2022%2f05-11-mcmc%2f&title=%e8%af%a6%e8%a7%a3Markov%20Chain%20Monte%20Carlo%20%28MCMC%29%3a%20%e4%bb%8e%e6%8b%92%e7%bb%9d-%e6%8e%a5%e5%8f%97%e9%87%87%e6%a0%b7%e5%88%b0Gibbs%20Sampling"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 详解Markov Chain Monte Carlo (MCMC): 从拒绝-接受采样到Gibbs Sampling on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fcoderlemon17.github.io%2fposts%2f2022%2f05-11-mcmc%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 详解Markov Chain Monte Carlo (MCMC): 从拒绝-接受采样到Gibbs Sampling on whatsapp" href="https://api.whatsapp.com/send?text=%e8%af%a6%e8%a7%a3Markov%20Chain%20Monte%20Carlo%20%28MCMC%29%3a%20%e4%bb%8e%e6%8b%92%e7%bb%9d-%e6%8e%a5%e5%8f%97%e9%87%87%e6%a0%b7%e5%88%b0Gibbs%20Sampling%20-%20https%3a%2f%2fcoderlemon17.github.io%2fposts%2f2022%2f05-11-mcmc%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 详解Markov Chain Monte Carlo (MCMC): 从拒绝-接受采样到Gibbs Sampling on telegram" href="https://telegram.me/share/url?text=%e8%af%a6%e8%a7%a3Markov%20Chain%20Monte%20Carlo%20%28MCMC%29%3a%20%e4%bb%8e%e6%8b%92%e7%bb%9d-%e6%8e%a5%e5%8f%97%e9%87%87%e6%a0%b7%e5%88%b0Gibbs%20Sampling&url=https%3a%2f%2fcoderlemon17.github.io%2fposts%2f2022%2f05-11-mcmc%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://coderlemon17.github.io/>Lemon's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>